{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "naive.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnT2xwCIbLWP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import os\n",
        "import numpy as np\n",
        "import json"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kc3kKlk_XG7P"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZUvCth2XKl2"
      },
      "source": [
        "class JsonDataset(data.Dataset):\n",
        "    def __init__(self, data_path):\n",
        "        f = open(data_path, 'r')\n",
        "        self.data = json.loads(f.read())\n",
        "        f.close()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return torch.FloatTensor(self.data[index][0]), \\\n",
        "            torch.LongTensor([float(self.data[index][1])])\n",
        "                # torch.FloatTensor(self.data[index][0]).sum()\n",
        "        #if self.data[index][0][-1] > 0.1:\n",
        "        #    return torch.FloatTensor(self.data[index][0]), torch.LongTensor([1])\n",
        "        #else:\n",
        "        #    return torch.FloatTensor(self.data[index][0]), torch.LongTensor([0])\n",
        "            \n",
        "            "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccExaxKIXs7u"
      },
      "source": [
        "train_data = JsonDataset('naive-train.json')\n",
        "validation_data = JsonDataset('naive-validation.json')\n",
        "test_data = JsonDataset('naive-test.json')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd4vUA9TX__H"
      },
      "source": [
        "input_size = train_data[0][0].shape[0]\n",
        "batch_size = 64\n",
        "fc_hidden_size = 100\n",
        "fc_num_layers = 5"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EnaXaTUirep"
      },
      "source": [
        "params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 1, 'pin_memory': True}\n",
        "train_loader = data.DataLoader(train_data, **params)\n",
        "validation_loader = data.DataLoader(validation_data, **params)\n",
        "test_loader = data.DataLoader(test_data, **params)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrUhCI6yjfLj"
      },
      "source": [
        "class Naive(nn.Module):\n",
        "    def __init__(self, input_size, fc_hidden_size, fc_num_layers):\n",
        "        super(Naive, self).__init__()\n",
        "\n",
        "        seq = []\n",
        "        seq.append(nn.Linear(input_size, fc_hidden_size))\n",
        "        seq.append(nn.Tanh())\n",
        "\n",
        "        for _ in range(fc_num_layers - 1):\n",
        "            seq.append(nn.Linear(fc_hidden_size, fc_hidden_size))\n",
        "            seq.append(nn.Tanh())\n",
        "            #seq.append(nn.Sigmoid())\n",
        "            seq.append(nn.Dropout(0.5))\n",
        "        \n",
        "        seq.append(nn.Linear(fc_hidden_size, 2))\n",
        "\n",
        "        self.fc = nn.Sequential(*seq)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc(x)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toxiizvhd-s6"
      },
      "source": [
        "def train(model, criterion, optimizer):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = torch.flatten(targets).to(device)\n",
        "        #targets = targets.reshape(-1, 1).to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += targets.size(0)\n",
        "        train_loss += loss.item() * targets.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "    epoch_train_loss = train_loss / total\n",
        "    epoch_train_acc = float(100 * correct / total)\n",
        "\n",
        "    return epoch_train_loss, epoch_train_acc"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR5S2dGD_ZTR"
      },
      "source": [
        "def validation(model, criterion):\n",
        "    model.eval()\n",
        "    validation_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, targets) in enumerate(validation_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = torch.flatten(targets).to(device)\n",
        "            #targets = targets.reshape(-1, 1).to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        total += targets.size(0)\n",
        "        validation_loss += loss.item() * targets.size(0)\n",
        "        _, predicted = outputs.max(1)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "    epoch_validation_loss = validation_loss / total\n",
        "    epoch_validation_acc = float(100 * correct / total)\n",
        "\n",
        "    return epoch_validation_loss, epoch_validation_acc"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj5YBGfkgeLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e70c5b20-969b-45d0-a44e-1ca24d0cfba1"
      },
      "source": [
        "weight_zero = len([i for i in range(len(train_data)) if train_data[i][1] == 1]) / len(train_data)\n",
        "print('weight_zero: {}'.format(weight_zero))\n",
        "\n",
        "model = Naive(input_size, fc_hidden_size, fc_num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.tensor([weight_zero, 1 - weight_zero]).to(device))\n",
        "#criterion = nn.CrossEntropyLoss().to(device)\n",
        "#criterion = nn.MSELoss().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  \n",
        "\n",
        "num_epochs = 100"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weight_zero: 0.49171875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-n1JdS_gD8c",
        "outputId": "c6a93907-4322-4880-ac80-7bcf9d72fd25"
      },
      "source": [
        "best_validation_loss = None\n",
        "\n",
        "for epoch in range(0, num_epochs):\n",
        "    epoch_train_loss, epoch_train_acc = train(model, criterion, optimizer)\n",
        "    epoch_validation_loss, epoch_validation_acc = validation(\n",
        "                                                    model, criterion)\n",
        "    \n",
        "    if best_validation_loss == None or epoch_validation_loss < best_validation_loss:\n",
        "        torch.save(model.state_dict(), 'best_naive.pth')\n",
        "        print('Saved.')\n",
        "        best_validation_loss = epoch_validation_loss\n",
        "\n",
        "    #epoch_test_loss, epoch_test_acc = test(net, criterion, vgg['best_acc'], 'vgg_best.pth')\n",
        "    #vgg['test_loss'].append(epoch_test_loss)\n",
        "    #vgg['test_acc'].append(epoch_test_acc)\n",
        "    #if epoch_test_acc > vgg['best_acc']:\n",
        "    #    vgg['best_acc'] = epoch_test_acc\n",
        "\n",
        "    print('Epoch {}. Training loss: {} ({}% accuracy). Validation loss: {} ({}% accuracy)'\n",
        "        .format(epoch + 1, \n",
        "                format(epoch_train_loss, '.4f'), format(epoch_train_acc, '.4f'),\n",
        "                format(epoch_validation_loss, '.4f'), format(epoch_validation_acc, '.4f')))\n",
        "    \n",
        "#print('Best test accuracy: {}%'.format(vgg['best_acc']))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved.\n",
            "Epoch 1. Training loss: 0.6939 (50.4219% accuracy). Validation loss: 0.6931 (48.4375% accuracy)\n",
            "Epoch 2. Training loss: 0.6940 (48.5781% accuracy). Validation loss: 0.6934 (50.0000% accuracy)\n",
            "Saved.\n",
            "Epoch 3. Training loss: 0.6929 (51.8125% accuracy). Validation loss: 0.6886 (54.6875% accuracy)\n",
            "Epoch 4. Training loss: 0.6847 (53.6875% accuracy). Validation loss: 0.7157 (56.2500% accuracy)\n",
            "Saved.\n",
            "Epoch 5. Training loss: 0.6308 (63.2031% accuracy). Validation loss: 0.6506 (64.0625% accuracy)\n",
            "Saved.\n",
            "Epoch 6. Training loss: 0.5964 (66.9688% accuracy). Validation loss: 0.5849 (73.4375% accuracy)\n",
            "Saved.\n",
            "Epoch 7. Training loss: 0.5738 (69.5000% accuracy). Validation loss: 0.4618 (82.8125% accuracy)\n",
            "Epoch 8. Training loss: 0.5526 (71.6875% accuracy). Validation loss: 0.5458 (73.4375% accuracy)\n",
            "Saved.\n",
            "Epoch 9. Training loss: 0.5423 (73.1406% accuracy). Validation loss: 0.4200 (84.3750% accuracy)\n",
            "Epoch 10. Training loss: 0.5334 (73.6562% accuracy). Validation loss: 0.4576 (79.6875% accuracy)\n",
            "Epoch 11. Training loss: 0.5291 (74.0312% accuracy). Validation loss: 0.4861 (75.0000% accuracy)\n",
            "Epoch 12. Training loss: 0.5090 (75.9531% accuracy). Validation loss: 0.5340 (76.5625% accuracy)\n",
            "Epoch 13. Training loss: 0.4912 (76.9062% accuracy). Validation loss: 0.5001 (71.8750% accuracy)\n",
            "Saved.\n",
            "Epoch 14. Training loss: 0.4877 (76.3906% accuracy). Validation loss: 0.4064 (81.2500% accuracy)\n",
            "Epoch 15. Training loss: 0.4837 (77.2500% accuracy). Validation loss: 0.4642 (76.5625% accuracy)\n",
            "Epoch 16. Training loss: 0.4763 (77.5625% accuracy). Validation loss: 0.4923 (73.4375% accuracy)\n",
            "Epoch 17. Training loss: 0.4602 (78.9062% accuracy). Validation loss: 0.4337 (82.8125% accuracy)\n",
            "Epoch 18. Training loss: 0.4556 (78.7500% accuracy). Validation loss: 0.4598 (79.6875% accuracy)\n",
            "Epoch 19. Training loss: 0.4452 (79.0156% accuracy). Validation loss: 0.4616 (75.0000% accuracy)\n",
            "Saved.\n",
            "Epoch 20. Training loss: 0.4387 (79.7500% accuracy). Validation loss: 0.3782 (82.8125% accuracy)\n",
            "Epoch 21. Training loss: 0.4359 (79.8438% accuracy). Validation loss: 0.4142 (78.1250% accuracy)\n",
            "Epoch 22. Training loss: 0.4290 (80.5312% accuracy). Validation loss: 0.5390 (75.0000% accuracy)\n",
            "Epoch 23. Training loss: 0.4207 (80.4688% accuracy). Validation loss: 0.4685 (81.2500% accuracy)\n",
            "Epoch 24. Training loss: 0.4105 (80.8594% accuracy). Validation loss: 0.4384 (79.6875% accuracy)\n",
            "Epoch 25. Training loss: 0.4067 (81.3438% accuracy). Validation loss: 0.4230 (76.5625% accuracy)\n",
            "Epoch 26. Training loss: 0.4078 (80.8906% accuracy). Validation loss: 0.3834 (81.2500% accuracy)\n",
            "Epoch 27. Training loss: 0.3970 (81.7344% accuracy). Validation loss: 0.5506 (78.1250% accuracy)\n",
            "Epoch 28. Training loss: 0.3931 (82.1406% accuracy). Validation loss: 0.4108 (79.6875% accuracy)\n",
            "Epoch 29. Training loss: 0.3900 (81.9375% accuracy). Validation loss: 0.5194 (82.8125% accuracy)\n",
            "Epoch 30. Training loss: 0.3915 (81.9375% accuracy). Validation loss: 0.6241 (75.0000% accuracy)\n",
            "Epoch 31. Training loss: 0.3829 (82.3438% accuracy). Validation loss: 0.5047 (81.2500% accuracy)\n",
            "Epoch 32. Training loss: 0.3849 (82.1875% accuracy). Validation loss: 0.5581 (70.3125% accuracy)\n",
            "Epoch 33. Training loss: 0.3771 (82.2656% accuracy). Validation loss: 0.5115 (79.6875% accuracy)\n",
            "Epoch 34. Training loss: 0.3696 (83.1875% accuracy). Validation loss: 0.5369 (81.2500% accuracy)\n",
            "Epoch 35. Training loss: 0.3742 (82.6875% accuracy). Validation loss: 0.6442 (70.3125% accuracy)\n",
            "Saved.\n",
            "Epoch 36. Training loss: 0.3691 (82.8750% accuracy). Validation loss: 0.3289 (87.5000% accuracy)\n",
            "Epoch 37. Training loss: 0.3696 (82.9062% accuracy). Validation loss: 0.3822 (81.2500% accuracy)\n",
            "Epoch 38. Training loss: 0.3603 (83.4844% accuracy). Validation loss: 0.5115 (70.3125% accuracy)\n",
            "Epoch 39. Training loss: 0.3467 (84.4531% accuracy). Validation loss: 0.3963 (79.6875% accuracy)\n",
            "Epoch 40. Training loss: 0.3441 (84.3125% accuracy). Validation loss: 0.4646 (78.1250% accuracy)\n",
            "Epoch 41. Training loss: 0.3417 (84.8750% accuracy). Validation loss: 0.4607 (76.5625% accuracy)\n",
            "Saved.\n",
            "Epoch 42. Training loss: 0.3441 (84.2031% accuracy). Validation loss: 0.2641 (90.6250% accuracy)\n",
            "Epoch 43. Training loss: 0.3462 (84.3281% accuracy). Validation loss: 0.3116 (85.9375% accuracy)\n",
            "Epoch 44. Training loss: 0.3378 (84.8125% accuracy). Validation loss: 0.4062 (82.8125% accuracy)\n",
            "Epoch 45. Training loss: 0.3342 (84.9062% accuracy). Validation loss: 0.3453 (85.9375% accuracy)\n",
            "Epoch 46. Training loss: 0.3293 (85.0469% accuracy). Validation loss: 0.3713 (79.6875% accuracy)\n",
            "Epoch 47. Training loss: 0.3300 (84.8125% accuracy). Validation loss: 0.7209 (75.0000% accuracy)\n",
            "Epoch 48. Training loss: 0.3267 (85.0781% accuracy). Validation loss: 0.5213 (78.1250% accuracy)\n",
            "Epoch 49. Training loss: 0.3306 (85.4062% accuracy). Validation loss: 0.5003 (79.6875% accuracy)\n",
            "Epoch 50. Training loss: 0.3168 (85.7500% accuracy). Validation loss: 0.5769 (73.4375% accuracy)\n",
            "Epoch 51. Training loss: 0.3270 (85.3125% accuracy). Validation loss: 0.6158 (76.5625% accuracy)\n",
            "Epoch 52. Training loss: 0.3129 (85.9219% accuracy). Validation loss: 0.3444 (82.8125% accuracy)\n",
            "Epoch 53. Training loss: 0.3132 (86.0000% accuracy). Validation loss: 0.5072 (73.4375% accuracy)\n",
            "Epoch 54. Training loss: 0.3050 (86.2969% accuracy). Validation loss: 0.5086 (82.8125% accuracy)\n",
            "Epoch 55. Training loss: 0.3020 (86.6719% accuracy). Validation loss: 0.3635 (79.6875% accuracy)\n",
            "Epoch 56. Training loss: 0.3067 (86.4062% accuracy). Validation loss: 0.3626 (84.3750% accuracy)\n",
            "Epoch 57. Training loss: 0.3078 (86.4844% accuracy). Validation loss: 0.3725 (81.2500% accuracy)\n",
            "Epoch 58. Training loss: 0.3039 (86.8906% accuracy). Validation loss: 0.4499 (81.2500% accuracy)\n",
            "Epoch 59. Training loss: 0.3023 (87.0000% accuracy). Validation loss: 0.6441 (79.6875% accuracy)\n",
            "Epoch 60. Training loss: 0.2856 (87.0156% accuracy). Validation loss: 0.4522 (84.3750% accuracy)\n",
            "Epoch 61. Training loss: 0.2919 (87.1250% accuracy). Validation loss: 0.7213 (73.4375% accuracy)\n",
            "Epoch 62. Training loss: 0.3029 (86.7656% accuracy). Validation loss: 0.5470 (76.5625% accuracy)\n",
            "Epoch 63. Training loss: 0.2898 (87.0312% accuracy). Validation loss: 0.2814 (85.9375% accuracy)\n",
            "Epoch 64. Training loss: 0.2880 (87.4688% accuracy). Validation loss: 0.3874 (81.2500% accuracy)\n",
            "Epoch 65. Training loss: 0.2777 (87.7344% accuracy). Validation loss: 0.7752 (76.5625% accuracy)\n",
            "Epoch 66. Training loss: 0.2859 (87.4219% accuracy). Validation loss: 0.7680 (79.6875% accuracy)\n",
            "Epoch 67. Training loss: 0.2871 (87.7656% accuracy). Validation loss: 0.4091 (84.3750% accuracy)\n",
            "Epoch 68. Training loss: 0.2821 (87.7500% accuracy). Validation loss: 0.2924 (85.9375% accuracy)\n",
            "Epoch 69. Training loss: 0.2730 (87.6562% accuracy). Validation loss: 0.2831 (92.1875% accuracy)\n",
            "Epoch 70. Training loss: 0.2783 (87.6562% accuracy). Validation loss: 0.5628 (79.6875% accuracy)\n",
            "Epoch 71. Training loss: 0.2836 (87.8906% accuracy). Validation loss: 0.5036 (85.9375% accuracy)\n",
            "Epoch 72. Training loss: 0.2768 (87.3750% accuracy). Validation loss: 0.3726 (79.6875% accuracy)\n",
            "Epoch 73. Training loss: 0.2759 (88.0938% accuracy). Validation loss: 0.5932 (73.4375% accuracy)\n",
            "Epoch 74. Training loss: 0.2617 (88.6406% accuracy). Validation loss: 0.8657 (71.8750% accuracy)\n",
            "Epoch 75. Training loss: 0.2584 (88.4062% accuracy). Validation loss: 0.5493 (76.5625% accuracy)\n",
            "Epoch 76. Training loss: 0.2621 (88.8906% accuracy). Validation loss: 0.3278 (85.9375% accuracy)\n",
            "Epoch 77. Training loss: 0.2596 (88.7344% accuracy). Validation loss: 0.3225 (89.0625% accuracy)\n",
            "Epoch 78. Training loss: 0.2718 (88.0625% accuracy). Validation loss: 0.5152 (79.6875% accuracy)\n",
            "Epoch 79. Training loss: 0.2661 (88.5938% accuracy). Validation loss: 0.4436 (82.8125% accuracy)\n",
            "Epoch 80. Training loss: 0.2691 (88.2969% accuracy). Validation loss: 0.8616 (67.1875% accuracy)\n",
            "Epoch 81. Training loss: 0.2655 (88.1719% accuracy). Validation loss: 0.3618 (81.2500% accuracy)\n",
            "Epoch 82. Training loss: 0.2638 (88.5469% accuracy). Validation loss: 0.5952 (79.6875% accuracy)\n",
            "Epoch 83. Training loss: 0.2584 (88.5938% accuracy). Validation loss: 0.4658 (89.0625% accuracy)\n",
            "Epoch 84. Training loss: 0.2489 (89.2656% accuracy). Validation loss: 0.5017 (71.8750% accuracy)\n",
            "Epoch 85. Training loss: 0.2572 (88.8125% accuracy). Validation loss: 0.6659 (81.2500% accuracy)\n",
            "Epoch 86. Training loss: 0.2598 (88.3906% accuracy). Validation loss: 0.4776 (78.1250% accuracy)\n",
            "Epoch 87. Training loss: 0.2460 (89.7500% accuracy). Validation loss: 0.6781 (78.1250% accuracy)\n",
            "Epoch 88. Training loss: 0.2579 (88.6875% accuracy). Validation loss: 1.0916 (76.5625% accuracy)\n",
            "Epoch 89. Training loss: 0.2634 (88.5625% accuracy). Validation loss: 0.7299 (82.8125% accuracy)\n",
            "Epoch 90. Training loss: 0.2519 (89.2188% accuracy). Validation loss: 0.4719 (85.9375% accuracy)\n",
            "Epoch 91. Training loss: 0.2451 (89.4062% accuracy). Validation loss: 0.4017 (85.9375% accuracy)\n",
            "Epoch 92. Training loss: 0.2404 (90.0000% accuracy). Validation loss: 0.8245 (79.6875% accuracy)\n",
            "Epoch 93. Training loss: 0.2461 (89.6562% accuracy). Validation loss: 0.6123 (81.2500% accuracy)\n",
            "Epoch 94. Training loss: 0.2330 (89.8438% accuracy). Validation loss: 0.9028 (79.6875% accuracy)\n",
            "Epoch 95. Training loss: 0.2439 (89.2344% accuracy). Validation loss: 0.4830 (82.8125% accuracy)\n",
            "Epoch 96. Training loss: 0.2319 (90.0625% accuracy). Validation loss: 0.4331 (89.0625% accuracy)\n",
            "Epoch 97. Training loss: 0.2428 (89.6250% accuracy). Validation loss: 0.6191 (76.5625% accuracy)\n",
            "Epoch 98. Training loss: 0.2436 (89.7812% accuracy). Validation loss: 0.4419 (79.6875% accuracy)\n",
            "Epoch 99. Training loss: 0.2195 (90.2812% accuracy). Validation loss: 0.5552 (81.2500% accuracy)\n",
            "Epoch 100. Training loss: 0.2281 (90.0469% accuracy). Validation loss: 0.7362 (85.9375% accuracy)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyECLCM7ltec"
      },
      "source": [
        "#print(len(model.fc))\n",
        "#model.fc[10].bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-dIIA2EhglE",
        "outputId": "e7e841e3-4b15-4fb0-ad79-0fed74c7cdd2"
      },
      "source": [
        "# Test\n",
        "model.load_state_dict(torch.load('best_naive.pth'))\n",
        "\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for i, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = torch.flatten(targets).to(device)\n",
        "        #targets = targets.reshape(-1, 1).to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        #if i == 0:\n",
        "            #print(outputs)\n",
        "            #print(predicted)\n",
        "            #print(targets)\n",
        "\n",
        "        n_samples += inputs.shape[0]\n",
        "        n_correct += (predicted == targets).sum().item()\n",
        "    \n",
        "    acc = float(100 * n_correct / n_samples)\n",
        "    print('Test accuracy: {}%'.format(acc))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 78.3%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMoplCxcleIN",
        "outputId": "7ac5011c-f417-45f7-b58e-0aaebe434d0a"
      },
      "source": [
        "\"\"\"\n",
        "# Train\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, targets) in enumerate(train_loader): \n",
        "        model.train()\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        #targets = torch.flatten(targets).to(device)\n",
        "        targets = targets.reshape(-1, 1).to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    print('Epoch {}. Loss: {}'.format(epoch, loss.item()))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0. Loss: 0.8501914143562317\n",
            "Epoch 1. Loss: 0.2044791579246521\n",
            "Epoch 2. Loss: 0.1138804629445076\n",
            "Epoch 3. Loss: 0.0778626948595047\n",
            "Epoch 4. Loss: 0.04106287285685539\n",
            "Epoch 5. Loss: 0.02865191176533699\n",
            "Epoch 6. Loss: 0.03518471121788025\n",
            "Epoch 7. Loss: 0.005256733391433954\n",
            "Epoch 8. Loss: 0.0062057580798864365\n",
            "Epoch 9. Loss: 0.002033299533650279\n",
            "Epoch 10. Loss: 0.001143027562648058\n",
            "Epoch 11. Loss: 0.00204146234318614\n",
            "Epoch 12. Loss: 0.00045666348887607455\n",
            "Epoch 13. Loss: 0.013698620721697807\n",
            "Epoch 14. Loss: 0.0010930895805358887\n",
            "Epoch 15. Loss: 0.009303990751504898\n",
            "Epoch 16. Loss: 0.015400063246488571\n",
            "Epoch 17. Loss: 0.009674233384430408\n",
            "Epoch 18. Loss: 0.013845495879650116\n",
            "Epoch 19. Loss: 0.002136755967512727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMXfmQps4MXZ",
        "outputId": "47d8caa5-3616-4808-e159-3ba48adc33d0"
      },
      "source": [
        "# Test\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for i, (inputs, targets) in enumerate(test_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = torch.flatten(targets).to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        if i == 0:\n",
        "            print(predicted)\n",
        "\n",
        "        n_samples += inputs.shape[0]\n",
        "        n_correct += (predicted == targets).sum().item()\n",
        "    \n",
        "    accu = 100.0 * n_correct / n_samples\n",
        "    print('Accuracy: {}%'.format(100 * n_correct / n_samples))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0,\n",
            "        1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0,\n",
            "        1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0], device='cuda:0')\n",
            "Accuracy: 49.5%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHq00Y-8eLkB"
      },
      "source": [
        "f = open('naive-train.json', 'r')\n",
        "a = json.loads(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epvO6Mg0iMc0",
        "outputId": "95baf418-2b39-460d-8ad1-fd35edb5261a"
      },
      "source": [
        "a = torch.randn(3, 5, requires_grad=True)\n",
        "a\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.0103,  0.2698, -0.6094, -2.6507,  0.7647],\n",
              "        [-0.3342, -0.4605, -0.7460,  1.0389, -1.7225],\n",
              "        [-1.3920, -2.3970,  1.5567, -0.0712,  0.9943]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSbIqxaqquRi",
        "outputId": "27e8fa56-c6ef-484d-d676-841cff6c6495"
      },
      "source": [
        "torch.flatten(torch.zeros(3, 1)).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    }
  ]
}