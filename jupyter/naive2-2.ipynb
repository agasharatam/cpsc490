{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport os\nimport numpy as np\nimport json","metadata":{"id":"HnT2xwCIbLWP","trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)","metadata":{"id":"Kc3kKlk_XG7P","trusted":true},"execution_count":133,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"class JsonDataset(data.Dataset):\n    def __init__(self, data_path):\n        super(JsonDataset, self).__init__()\n        \n        f = open(data_path, 'r')\n        self.data = json.loads(f.read())\n        f.close()\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        return torch.FloatTensor(self.data[index][0]), \\\n            torch.FloatTensor(self.data[index][1]), \\\n            torch.FloatTensor(self.data[index][2]), \\\n            torch.LongTensor([float(self.data[index][3])])\n","metadata":{"id":"5ZUvCth2XKl2","trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"train_data = JsonDataset('/kaggle/input/cpsc490/naive2-train.json')\nvalidation_data = JsonDataset('/kaggle/input/cpsc490/naive2-validation.json')\ntest_data = JsonDataset('/kaggle/input/cpsc490/naive2-test.json')","metadata":{"id":"ccExaxKIXs7u","trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"x_center_num = train_data[0][0].shape[0]\nx_center_size = train_data[0][0].shape[1]\nx_size = train_data[0][2].shape[0]\n\nbatch_size = 256\nfc_hidden_size_1 = 100\nfc_hidden_size_2 = 5\nfc_hidden_size_3 = 100\nfc_num_layers = 2","metadata":{"id":"Fd4vUA9TX__H","trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 1, 'pin_memory': True}\ntrain_loader = data.DataLoader(train_data, **params)\nvalidation_loader = data.DataLoader(validation_data, **params)\ntest_loader = data.DataLoader(test_data, **params)","metadata":{"id":"-EnaXaTUirep","trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"class Naive(nn.Module):\n    def __init__(self, x_center_num, x_center_size, x_size, \\\n                 fc_hidden_size_1, fc_hidden_size_2, fc_hidden_size_3, \\\n                 fc_num_layers):\n        super(Naive, self).__init__()\n        \n        d = 0.5\n        \n        self.x_center_num = x_center_num\n        \n        seq = []\n        seq.append(nn.Linear(x_center_size + x_size, fc_hidden_size_1))\n        seq.append(nn.Tanh())\n        seq.append(nn.Dropout(d))\n        seq.append(nn.Linear(fc_hidden_size_1, 1))\n        self.fc1 = nn.Sequential(*seq)\n        \n        seq = []\n        seq.append(nn.Linear(2, 1))\n        #seq.append(nn.Tanh())\n        #seq.append(nn.Dropout(d))\n        #seq.append(nn.Linear(fc_hidden_size_2, 1))\n        self.fc2 = nn.Sequential(*seq)\n\n        seq = []\n        seq.append(nn.Linear(x_center_num, fc_hidden_size_3))\n        seq.append(nn.Tanh())\n        seq.append(nn.Dropout(d))\n\n        for _ in range(fc_num_layers - 1):\n            seq.append(nn.Linear(fc_hidden_size_3, fc_hidden_size_3))\n            seq.append(nn.Tanh())\n            seq.append(nn.Dropout(d))\n        \n        seq.append(nn.Linear(fc_hidden_size_3, 2))\n        self.fc3 = nn.Sequential(*seq)\n        \n    def forward(self, x_center, x_real, x):\n        layer = None\n        for i in range(self.x_center_num):\n            res1 = self.fc1(torch.cat((x_center[:, i, :], x), dim=1))\n            \n            res2 = self.fc2(torch.cat((res1, x_real[:, i].reshape(-1, 1)), dim=1))\n            \n            if layer == None:\n                layer = res2\n            else:\n                layer = torch.cat((layer, res2), dim=1)\n                \n        out = self.fc3(layer)\n        return out","metadata":{"id":"GrUhCI6yjfLj","trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"def train(model, criterion, optimizer):\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n\n    for i, (x_center, x_real, x, targets) in enumerate(train_loader):\n        x_center = x_center.to(device)\n        x_real = x_real.to(device)\n        x = x.to(device)\n        targets = torch.flatten(targets).to(device)\n\n        outputs = model(x_center, x_real, x)\n        loss = criterion(outputs, targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total += targets.size(0)\n        train_loss += loss.item() * targets.size(0)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(targets).sum().item()\n        \n    epoch_train_loss = train_loss / total\n    epoch_train_acc = float(100 * correct / total)\n\n    return epoch_train_loss, epoch_train_acc","metadata":{"id":"toxiizvhd-s6","trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"def validation(model, criterion):\n    model.eval()\n    validation_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for i, (x_center, x_real, x, targets) in enumerate(validation_loader):\n            x_center = x_center.to(device)\n            x_real = x_real.to(device)\n            x = x.to(device)\n            targets = torch.flatten(targets).to(device)\n\n            outputs = model(x_center, x_real, x)\n            loss = criterion(outputs, targets)\n\n        total += targets.size(0)\n        validation_loss += loss.item() * targets.size(0)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(targets).sum().item()\n        \n    epoch_validation_loss = validation_loss / total\n    epoch_validation_acc = float(100 * correct / total)\n\n    return epoch_validation_loss, epoch_validation_acc","metadata":{"id":"vR5S2dGD_ZTR","trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"fraction_zero = len([i for i in range(len(train_data)) if train_data[i][3] == 0]) / len(train_data)\nprint('fraction_zero: {}'.format(fraction_zero))\n\nmodel = Naive(x_center_num, x_center_size, x_size, \\\n                 fc_hidden_size_1, fc_hidden_size_2, fc_hidden_size_3, \\\n                 fc_num_layers).to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=torch.tensor([0.5 / fraction_zero, 0.5 / (1 - fraction_zero)]).to(device))\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n#optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0002)\n\nnum_epochs = 200","metadata":{"id":"Sj5YBGfkgeLE","outputId":"e70c5b20-969b-45d0-a44e-1ca24d0cfba1","trusted":true},"execution_count":141,"outputs":[{"name":"stdout","text":"fraction_zero: 0.4996875\n","output_type":"stream"}]},{"cell_type":"code","source":"best_validation_loss = None\n\nfor epoch in range(0, num_epochs):\n    epoch_train_loss, epoch_train_acc = train(model, criterion, optimizer)\n    epoch_validation_loss, epoch_validation_acc = validation(\n                                                    model, criterion)\n    \n    if best_validation_loss == None or epoch_validation_loss < best_validation_loss:\n        torch.save(model.state_dict(), 'best_naive2.pth')\n        print('Saved.')\n        best_validation_loss = epoch_validation_loss\n\n    #epoch_test_loss, epoch_test_acc = test(net, criterion, vgg['best_acc'], 'vgg_best.pth')\n    #vgg['test_loss'].append(epoch_test_loss)\n    #vgg['test_acc'].append(epoch_test_acc)\n    #if epoch_test_acc > vgg['best_acc']:\n    #    vgg['best_acc'] = epoch_test_acc\n\n    print('Epoch {}. Training loss: {} ({}% accuracy). Validation loss: {} ({}% accuracy)'\n        .format(epoch + 1, \n                format(epoch_train_loss, '.4f'), format(epoch_train_acc, '.4f'),\n                format(epoch_validation_loss, '.4f'), format(epoch_validation_acc, '.4f')))","metadata":{"id":"7-n1JdS_gD8c","outputId":"c6a93907-4322-4880-ac80-7bcf9d72fd25","trusted":true},"execution_count":142,"outputs":[{"name":"stdout","text":"Saved.\nEpoch 1. Training loss: 0.7111 (50.6094% accuracy). Validation loss: 0.7030 (45.3125% accuracy)\nSaved.\nEpoch 2. Training loss: 0.6959 (51.1875% accuracy). Validation loss: 0.6833 (54.6875% accuracy)\nEpoch 3. Training loss: 0.6942 (50.9375% accuracy). Validation loss: 0.6833 (64.0625% accuracy)\nEpoch 4. Training loss: 0.6933 (50.5156% accuracy). Validation loss: 0.6887 (56.2500% accuracy)\nEpoch 5. Training loss: 0.6932 (51.9531% accuracy). Validation loss: 0.6967 (42.1875% accuracy)\nEpoch 6. Training loss: 0.6932 (52.1562% accuracy). Validation loss: 0.7164 (42.1875% accuracy)\nSaved.\nEpoch 7. Training loss: 0.6967 (50.3125% accuracy). Validation loss: 0.6789 (60.9375% accuracy)\nEpoch 8. Training loss: 0.6947 (51.3438% accuracy). Validation loss: 0.6945 (51.5625% accuracy)\nEpoch 9. Training loss: 0.6943 (50.7344% accuracy). Validation loss: 0.6846 (53.1250% accuracy)\nEpoch 10. Training loss: 0.6967 (49.7969% accuracy). Validation loss: 0.6918 (50.0000% accuracy)\nEpoch 11. Training loss: 0.6951 (50.1875% accuracy). Validation loss: 0.6823 (59.3750% accuracy)\nEpoch 12. Training loss: 0.6939 (51.3906% accuracy). Validation loss: 0.6903 (56.2500% accuracy)\nEpoch 13. Training loss: 0.6947 (51.2969% accuracy). Validation loss: 0.6999 (50.0000% accuracy)\nEpoch 14. Training loss: 0.6943 (50.5469% accuracy). Validation loss: 0.6931 (48.4375% accuracy)\nSaved.\nEpoch 15. Training loss: 0.6898 (52.4688% accuracy). Validation loss: 0.6504 (67.1875% accuracy)\nEpoch 16. Training loss: 0.6899 (53.0469% accuracy). Validation loss: 0.6945 (45.3125% accuracy)\nEpoch 17. Training loss: 0.6772 (55.8594% accuracy). Validation loss: 0.6876 (50.0000% accuracy)\nEpoch 18. Training loss: 0.6687 (56.7969% accuracy). Validation loss: 0.6682 (50.0000% accuracy)\nEpoch 19. Training loss: 0.6599 (60.0625% accuracy). Validation loss: 0.6984 (59.3750% accuracy)\nSaved.\nEpoch 20. Training loss: 0.6501 (60.2812% accuracy). Validation loss: 0.6458 (65.6250% accuracy)\nSaved.\nEpoch 21. Training loss: 0.6278 (64.1406% accuracy). Validation loss: 0.5944 (65.6250% accuracy)\nEpoch 22. Training loss: 0.6121 (67.9062% accuracy). Validation loss: 0.6742 (53.1250% accuracy)\nEpoch 23. Training loss: 0.6152 (65.5156% accuracy). Validation loss: 0.6045 (71.8750% accuracy)\nSaved.\nEpoch 24. Training loss: 0.5911 (69.8750% accuracy). Validation loss: 0.5302 (68.7500% accuracy)\nEpoch 25. Training loss: 0.5687 (71.2500% accuracy). Validation loss: 0.5838 (68.7500% accuracy)\nEpoch 26. Training loss: 0.5715 (71.1094% accuracy). Validation loss: 0.5756 (70.3125% accuracy)\nEpoch 27. Training loss: 0.5756 (71.1406% accuracy). Validation loss: 0.5424 (71.8750% accuracy)\nSaved.\nEpoch 28. Training loss: 0.5842 (70.1562% accuracy). Validation loss: 0.4810 (84.3750% accuracy)\nEpoch 29. Training loss: 0.5635 (72.5156% accuracy). Validation loss: 0.5485 (71.8750% accuracy)\nEpoch 30. Training loss: 0.5653 (71.7031% accuracy). Validation loss: 0.5559 (70.3125% accuracy)\nEpoch 31. Training loss: 0.5575 (72.7500% accuracy). Validation loss: 0.5635 (78.1250% accuracy)\nSaved.\nEpoch 32. Training loss: 0.5583 (72.7969% accuracy). Validation loss: 0.4625 (76.5625% accuracy)\nEpoch 33. Training loss: 0.5542 (73.2188% accuracy). Validation loss: 0.5625 (71.8750% accuracy)\nEpoch 34. Training loss: 0.5533 (72.4688% accuracy). Validation loss: 0.5088 (73.4375% accuracy)\nEpoch 35. Training loss: 0.5518 (72.8438% accuracy). Validation loss: 0.6943 (59.3750% accuracy)\nEpoch 36. Training loss: 0.5525 (72.8125% accuracy). Validation loss: 0.5107 (76.5625% accuracy)\nEpoch 37. Training loss: 0.5518 (73.4375% accuracy). Validation loss: 0.5371 (71.8750% accuracy)\nEpoch 38. Training loss: 0.5465 (73.6562% accuracy). Validation loss: 0.4895 (79.6875% accuracy)\nEpoch 39. Training loss: 0.5515 (72.7031% accuracy). Validation loss: 0.5200 (73.4375% accuracy)\nEpoch 40. Training loss: 0.5478 (72.7344% accuracy). Validation loss: 0.5436 (68.7500% accuracy)\nEpoch 41. Training loss: 0.5393 (73.8906% accuracy). Validation loss: 0.5620 (71.8750% accuracy)\nEpoch 42. Training loss: 0.5432 (73.2500% accuracy). Validation loss: 0.6093 (70.3125% accuracy)\nEpoch 43. Training loss: 0.5392 (73.8906% accuracy). Validation loss: 0.5660 (64.0625% accuracy)\nEpoch 44. Training loss: 0.5437 (73.2656% accuracy). Validation loss: 0.5492 (75.0000% accuracy)\nEpoch 45. Training loss: 0.5436 (73.2031% accuracy). Validation loss: 0.5922 (73.4375% accuracy)\nEpoch 46. Training loss: 0.5574 (72.5156% accuracy). Validation loss: 0.6090 (70.3125% accuracy)\nSaved.\nEpoch 47. Training loss: 0.5524 (72.9062% accuracy). Validation loss: 0.3659 (84.3750% accuracy)\nEpoch 48. Training loss: 0.5473 (73.0938% accuracy). Validation loss: 0.5988 (65.6250% accuracy)\nEpoch 49. Training loss: 0.5428 (73.3281% accuracy). Validation loss: 0.5055 (71.8750% accuracy)\nEpoch 50. Training loss: 0.5405 (73.3125% accuracy). Validation loss: 0.5699 (70.3125% accuracy)\nEpoch 51. Training loss: 0.5360 (73.4688% accuracy). Validation loss: 0.6251 (70.3125% accuracy)\nEpoch 52. Training loss: 0.5453 (72.9531% accuracy). Validation loss: 0.4737 (76.5625% accuracy)\nEpoch 53. Training loss: 0.5324 (73.5625% accuracy). Validation loss: 0.6567 (67.1875% accuracy)\nEpoch 54. Training loss: 0.5360 (73.4062% accuracy). Validation loss: 0.7270 (60.9375% accuracy)\nEpoch 55. Training loss: 0.5470 (73.4531% accuracy). Validation loss: 0.4742 (76.5625% accuracy)\nEpoch 56. Training loss: 0.5403 (73.3594% accuracy). Validation loss: 0.6125 (67.1875% accuracy)\nEpoch 57. Training loss: 0.5428 (72.8594% accuracy). Validation loss: 0.5680 (68.7500% accuracy)\nEpoch 58. Training loss: 0.5393 (73.7656% accuracy). Validation loss: 0.5635 (73.4375% accuracy)\nEpoch 59. Training loss: 0.5357 (74.1250% accuracy). Validation loss: 0.7299 (62.5000% accuracy)\nEpoch 60. Training loss: 0.5546 (72.2812% accuracy). Validation loss: 0.6733 (67.1875% accuracy)\nEpoch 61. Training loss: 0.5521 (73.2812% accuracy). Validation loss: 0.4894 (79.6875% accuracy)\nEpoch 62. Training loss: 0.5362 (73.7344% accuracy). Validation loss: 0.6063 (67.1875% accuracy)\nEpoch 63. Training loss: 0.5384 (74.1094% accuracy). Validation loss: 0.5914 (70.3125% accuracy)\nEpoch 64. Training loss: 0.5375 (73.4219% accuracy). Validation loss: 0.5523 (71.8750% accuracy)\nEpoch 65. Training loss: 0.5377 (73.7344% accuracy). Validation loss: 0.6995 (64.0625% accuracy)\nEpoch 66. Training loss: 0.5383 (73.8750% accuracy). Validation loss: 0.6607 (71.8750% accuracy)\nEpoch 67. Training loss: 0.5303 (73.9531% accuracy). Validation loss: 0.5300 (75.0000% accuracy)\nEpoch 68. Training loss: 0.5382 (73.2969% accuracy). Validation loss: 0.4416 (76.5625% accuracy)\nEpoch 69. Training loss: 0.5285 (73.9844% accuracy). Validation loss: 0.5062 (79.6875% accuracy)\nEpoch 70. Training loss: 0.5274 (74.3281% accuracy). Validation loss: 0.5800 (70.3125% accuracy)\nEpoch 71. Training loss: 0.5355 (73.7031% accuracy). Validation loss: 0.6108 (67.1875% accuracy)\nEpoch 72. Training loss: 0.5290 (73.9375% accuracy). Validation loss: 0.5555 (71.8750% accuracy)\nEpoch 73. Training loss: 0.5424 (73.0156% accuracy). Validation loss: 0.5979 (71.8750% accuracy)\nEpoch 74. Training loss: 0.5311 (74.0625% accuracy). Validation loss: 0.5025 (76.5625% accuracy)\nEpoch 75. Training loss: 0.5317 (73.7500% accuracy). Validation loss: 0.4222 (82.8125% accuracy)\nEpoch 76. Training loss: 0.5427 (73.3594% accuracy). Validation loss: 0.5543 (71.8750% accuracy)\nEpoch 77. Training loss: 0.5401 (73.4219% accuracy). Validation loss: 0.5213 (71.8750% accuracy)\nEpoch 78. Training loss: 0.5280 (74.2969% accuracy). Validation loss: 0.5692 (71.8750% accuracy)\nEpoch 79. Training loss: 0.5350 (73.5938% accuracy). Validation loss: 0.6673 (67.1875% accuracy)\nEpoch 80. Training loss: 0.5343 (73.5000% accuracy). Validation loss: 0.4645 (82.8125% accuracy)\nEpoch 81. Training loss: 0.5352 (73.7969% accuracy). Validation loss: 0.4868 (76.5625% accuracy)\nEpoch 82. Training loss: 0.5277 (74.2188% accuracy). Validation loss: 0.5657 (64.0625% accuracy)\nEpoch 83. Training loss: 0.5371 (73.3125% accuracy). Validation loss: 0.6036 (65.6250% accuracy)\nEpoch 84. Training loss: 0.5353 (73.2188% accuracy). Validation loss: 0.7276 (59.3750% accuracy)\nEpoch 85. Training loss: 0.5265 (74.5938% accuracy). Validation loss: 0.5534 (68.7500% accuracy)\nEpoch 86. Training loss: 0.5328 (74.0000% accuracy). Validation loss: 0.6435 (60.9375% accuracy)\nEpoch 87. Training loss: 0.5328 (73.8906% accuracy). Validation loss: 0.4653 (81.2500% accuracy)\nEpoch 88. Training loss: 0.5272 (74.2031% accuracy). Validation loss: 0.5190 (71.8750% accuracy)\nEpoch 89. Training loss: 0.5305 (73.9062% accuracy). Validation loss: 0.6717 (64.0625% accuracy)\nEpoch 90. Training loss: 0.5329 (73.7188% accuracy). Validation loss: 0.6236 (64.0625% accuracy)\nEpoch 91. Training loss: 0.5352 (73.3438% accuracy). Validation loss: 0.6184 (60.9375% accuracy)\nEpoch 92. Training loss: 0.5394 (72.7031% accuracy). Validation loss: 0.5375 (71.8750% accuracy)\nEpoch 93. Training loss: 0.5325 (73.8438% accuracy). Validation loss: 0.5401 (70.3125% accuracy)\nEpoch 94. Training loss: 0.5321 (73.9219% accuracy). Validation loss: 0.5010 (76.5625% accuracy)\nEpoch 95. Training loss: 0.5290 (73.9844% accuracy). Validation loss: 0.5239 (79.6875% accuracy)\nEpoch 96. Training loss: 0.5270 (73.7812% accuracy). Validation loss: 0.4774 (78.1250% accuracy)\nEpoch 97. Training loss: 0.5388 (73.3125% accuracy). Validation loss: 0.5201 (76.5625% accuracy)\nEpoch 98. Training loss: 0.5346 (73.7969% accuracy). Validation loss: 0.6195 (70.3125% accuracy)\nEpoch 99. Training loss: 0.5339 (73.8750% accuracy). Validation loss: 0.5213 (79.6875% accuracy)\nEpoch 100. Training loss: 0.5398 (73.7344% accuracy). Validation loss: 0.5737 (68.7500% accuracy)\nEpoch 101. Training loss: 0.5414 (73.1250% accuracy). Validation loss: 0.4962 (81.2500% accuracy)\nEpoch 102. Training loss: 0.5446 (73.0000% accuracy). Validation loss: 0.6143 (67.1875% accuracy)\nEpoch 103. Training loss: 0.5276 (74.0000% accuracy). Validation loss: 0.5910 (68.7500% accuracy)\nEpoch 104. Training loss: 0.5349 (73.7344% accuracy). Validation loss: 0.4742 (76.5625% accuracy)\nEpoch 105. Training loss: 0.5249 (74.6719% accuracy). Validation loss: 0.5429 (68.7500% accuracy)\nEpoch 106. Training loss: 0.5438 (73.1250% accuracy). Validation loss: 0.4830 (75.0000% accuracy)\nEpoch 107. Training loss: 0.5362 (73.6406% accuracy). Validation loss: 0.6252 (68.7500% accuracy)\nEpoch 108. Training loss: 0.5367 (73.3594% accuracy). Validation loss: 0.5375 (75.0000% accuracy)\nEpoch 109. Training loss: 0.5513 (72.4531% accuracy). Validation loss: 0.5679 (71.8750% accuracy)\nEpoch 110. Training loss: 0.5399 (73.1406% accuracy). Validation loss: 0.5015 (75.0000% accuracy)\nEpoch 111. Training loss: 0.5312 (73.7812% accuracy). Validation loss: 0.6265 (57.8125% accuracy)\nEpoch 112. Training loss: 0.5296 (73.8281% accuracy). Validation loss: 0.5871 (68.7500% accuracy)\nEpoch 113. Training loss: 0.5415 (73.5000% accuracy). Validation loss: 0.5870 (65.6250% accuracy)\nEpoch 114. Training loss: 0.5366 (73.5469% accuracy). Validation loss: 0.6260 (64.0625% accuracy)\nEpoch 115. Training loss: 0.5282 (74.2344% accuracy). Validation loss: 0.6125 (68.7500% accuracy)\nEpoch 116. Training loss: 0.5250 (74.2969% accuracy). Validation loss: 0.5645 (68.7500% accuracy)\nEpoch 117. Training loss: 0.5274 (73.9375% accuracy). Validation loss: 0.5272 (75.0000% accuracy)\nEpoch 118. Training loss: 0.5307 (73.9531% accuracy). Validation loss: 0.4489 (75.0000% accuracy)\nEpoch 119. Training loss: 0.5272 (74.1875% accuracy). Validation loss: 0.5285 (76.5625% accuracy)\nEpoch 120. Training loss: 0.5296 (73.7031% accuracy). Validation loss: 0.5397 (70.3125% accuracy)\nEpoch 121. Training loss: 0.5312 (73.7344% accuracy). Validation loss: 0.5703 (68.7500% accuracy)\nEpoch 122. Training loss: 0.5283 (74.0000% accuracy). Validation loss: 0.5579 (67.1875% accuracy)\nEpoch 123. Training loss: 0.5313 (74.0312% accuracy). Validation loss: 0.5065 (81.2500% accuracy)\nEpoch 124. Training loss: 0.5315 (74.1719% accuracy). Validation loss: 0.5976 (67.1875% accuracy)\nEpoch 125. Training loss: 0.5326 (73.6406% accuracy). Validation loss: 0.5506 (75.0000% accuracy)\nEpoch 126. Training loss: 0.5290 (73.4062% accuracy). Validation loss: 0.5655 (71.8750% accuracy)\nEpoch 127. Training loss: 0.5268 (73.9531% accuracy). Validation loss: 0.4525 (79.6875% accuracy)\nEpoch 128. Training loss: 0.5244 (74.0625% accuracy). Validation loss: 0.6476 (65.6250% accuracy)\nEpoch 129. Training loss: 0.5306 (74.2344% accuracy). Validation loss: 0.5152 (75.0000% accuracy)\nEpoch 130. Training loss: 0.5309 (73.7344% accuracy). Validation loss: 0.6026 (68.7500% accuracy)\nEpoch 131. Training loss: 0.5362 (73.9688% accuracy). Validation loss: 0.5622 (70.3125% accuracy)\nEpoch 132. Training loss: 0.5340 (74.3750% accuracy). Validation loss: 0.6429 (62.5000% accuracy)\nEpoch 133. Training loss: 0.5422 (72.9688% accuracy). Validation loss: 0.5255 (75.0000% accuracy)\nEpoch 134. Training loss: 0.5235 (74.1094% accuracy). Validation loss: 0.5953 (67.1875% accuracy)\nEpoch 135. Training loss: 0.5238 (74.0938% accuracy). Validation loss: 0.5320 (70.3125% accuracy)\nEpoch 136. Training loss: 0.5271 (73.8906% accuracy). Validation loss: 0.5717 (70.3125% accuracy)\nEpoch 137. Training loss: 0.5305 (73.9688% accuracy). Validation loss: 0.5039 (75.0000% accuracy)\nEpoch 138. Training loss: 0.5304 (73.8438% accuracy). Validation loss: 0.6084 (70.3125% accuracy)\nEpoch 139. Training loss: 0.5429 (73.1875% accuracy). Validation loss: 0.5529 (76.5625% accuracy)\nEpoch 140. Training loss: 0.5321 (73.8125% accuracy). Validation loss: 0.5110 (75.0000% accuracy)\nEpoch 141. Training loss: 0.5308 (74.3438% accuracy). Validation loss: 0.5909 (60.9375% accuracy)\nEpoch 142. Training loss: 0.5312 (74.1719% accuracy). Validation loss: 0.4892 (79.6875% accuracy)\nEpoch 143. Training loss: 0.5275 (74.0312% accuracy). Validation loss: 0.4849 (75.0000% accuracy)\nEpoch 144. Training loss: 0.5245 (74.6250% accuracy). Validation loss: 0.4998 (73.4375% accuracy)\nEpoch 145. Training loss: 0.5291 (73.4844% accuracy). Validation loss: 0.7784 (60.9375% accuracy)\nEpoch 146. Training loss: 0.5273 (74.4219% accuracy). Validation loss: 0.7076 (60.9375% accuracy)\nEpoch 147. Training loss: 0.5374 (73.7969% accuracy). Validation loss: 0.5055 (75.0000% accuracy)\nEpoch 148. Training loss: 0.5284 (73.9844% accuracy). Validation loss: 0.6261 (67.1875% accuracy)\nEpoch 149. Training loss: 0.5318 (73.6719% accuracy). Validation loss: 0.5550 (70.3125% accuracy)\nEpoch 150. Training loss: 0.5253 (74.3125% accuracy). Validation loss: 0.6874 (75.0000% accuracy)\nEpoch 151. Training loss: 0.5409 (73.8906% accuracy). Validation loss: 0.4915 (71.8750% accuracy)\nEpoch 152. Training loss: 0.5306 (73.9062% accuracy). Validation loss: 0.5794 (68.7500% accuracy)\nEpoch 153. Training loss: 0.5286 (73.7344% accuracy). Validation loss: 0.6093 (71.8750% accuracy)\nEpoch 154. Training loss: 0.5325 (73.5156% accuracy). Validation loss: 0.4623 (81.2500% accuracy)\nEpoch 155. Training loss: 0.5262 (74.1406% accuracy). Validation loss: 0.6006 (68.7500% accuracy)\nEpoch 156. Training loss: 0.5301 (73.6094% accuracy). Validation loss: 0.5593 (70.3125% accuracy)\nEpoch 157. Training loss: 0.5264 (74.2969% accuracy). Validation loss: 0.6654 (59.3750% accuracy)\nEpoch 158. Training loss: 0.5370 (74.0625% accuracy). Validation loss: 0.5504 (68.7500% accuracy)\nEpoch 159. Training loss: 0.5333 (73.4219% accuracy). Validation loss: 0.5848 (73.4375% accuracy)\nEpoch 160. Training loss: 0.5261 (74.4219% accuracy). Validation loss: 0.6195 (65.6250% accuracy)\nEpoch 161. Training loss: 0.5330 (74.1875% accuracy). Validation loss: 0.6393 (64.0625% accuracy)\nEpoch 162. Training loss: 0.5299 (73.7656% accuracy). Validation loss: 0.5386 (67.1875% accuracy)\nEpoch 163. Training loss: 0.5414 (73.0938% accuracy). Validation loss: 0.4827 (78.1250% accuracy)\nEpoch 164. Training loss: 0.5424 (72.7969% accuracy). Validation loss: 0.5052 (81.2500% accuracy)\nEpoch 165. Training loss: 0.5418 (73.0781% accuracy). Validation loss: 0.5809 (68.7500% accuracy)\nEpoch 166. Training loss: 0.5365 (73.5781% accuracy). Validation loss: 0.4959 (71.8750% accuracy)\nEpoch 167. Training loss: 0.5336 (73.6719% accuracy). Validation loss: 0.5437 (73.4375% accuracy)\nEpoch 168. Training loss: 0.5362 (73.5156% accuracy). Validation loss: 0.6164 (65.6250% accuracy)\nEpoch 169. Training loss: 0.5370 (73.6875% accuracy). Validation loss: 0.5693 (62.5000% accuracy)\nEpoch 170. Training loss: 0.5364 (73.5000% accuracy). Validation loss: 0.6301 (67.1875% accuracy)\nEpoch 171. Training loss: 0.5266 (74.2500% accuracy). Validation loss: 0.5677 (68.7500% accuracy)\nEpoch 172. Training loss: 0.5311 (74.0312% accuracy). Validation loss: 0.5705 (71.8750% accuracy)\nEpoch 173. Training loss: 0.5334 (73.6406% accuracy). Validation loss: 0.5177 (73.4375% accuracy)\nEpoch 174. Training loss: 0.5456 (73.5938% accuracy). Validation loss: 0.5515 (73.4375% accuracy)\nEpoch 175. Training loss: 0.5270 (74.3750% accuracy). Validation loss: 0.6641 (57.8125% accuracy)\nEpoch 176. Training loss: 0.5216 (74.4062% accuracy). Validation loss: 0.6243 (70.3125% accuracy)\nEpoch 177. Training loss: 0.5288 (73.9688% accuracy). Validation loss: 0.5399 (67.1875% accuracy)\nEpoch 178. Training loss: 0.5307 (73.6562% accuracy). Validation loss: 0.5332 (71.8750% accuracy)\nEpoch 179. Training loss: 0.5336 (73.8750% accuracy). Validation loss: 0.5062 (79.6875% accuracy)\nEpoch 180. Training loss: 0.5349 (73.6250% accuracy). Validation loss: 0.5721 (73.4375% accuracy)\nEpoch 181. Training loss: 0.5334 (73.3750% accuracy). Validation loss: 0.5651 (75.0000% accuracy)\nEpoch 182. Training loss: 0.5356 (73.6562% accuracy). Validation loss: 0.5775 (64.0625% accuracy)\nEpoch 183. Training loss: 0.5325 (73.6406% accuracy). Validation loss: 0.5972 (71.8750% accuracy)\nEpoch 184. Training loss: 0.5271 (74.0469% accuracy). Validation loss: 0.5710 (71.8750% accuracy)\nEpoch 185. Training loss: 0.5302 (73.8750% accuracy). Validation loss: 0.5886 (67.1875% accuracy)\nEpoch 186. Training loss: 0.5216 (74.0000% accuracy). Validation loss: 0.5801 (73.4375% accuracy)\nEpoch 187. Training loss: 0.5215 (74.5312% accuracy). Validation loss: 0.7110 (68.7500% accuracy)\nEpoch 188. Training loss: 0.5256 (74.4375% accuracy). Validation loss: 0.5457 (76.5625% accuracy)\nEpoch 189. Training loss: 0.5259 (73.8438% accuracy). Validation loss: 0.5736 (70.3125% accuracy)\nEpoch 190. Training loss: 0.5263 (74.2812% accuracy). Validation loss: 0.5277 (75.0000% accuracy)\nEpoch 191. Training loss: 0.5287 (74.1562% accuracy). Validation loss: 0.5938 (65.6250% accuracy)\nEpoch 192. Training loss: 0.5347 (73.6875% accuracy). Validation loss: 0.4527 (87.5000% accuracy)\nEpoch 193. Training loss: 0.5339 (73.5938% accuracy). Validation loss: 0.5716 (65.6250% accuracy)\nEpoch 194. Training loss: 0.5258 (74.6719% accuracy). Validation loss: 0.5820 (70.3125% accuracy)\nEpoch 195. Training loss: 0.5320 (73.6875% accuracy). Validation loss: 0.4752 (78.1250% accuracy)\nEpoch 196. Training loss: 0.5253 (74.0938% accuracy). Validation loss: 0.5908 (68.7500% accuracy)\nEpoch 197. Training loss: 0.5373 (73.7031% accuracy). Validation loss: 0.5855 (71.8750% accuracy)\nEpoch 198. Training loss: 0.5319 (74.3750% accuracy). Validation loss: 0.5356 (73.4375% accuracy)\nEpoch 199. Training loss: 0.5285 (73.9062% accuracy). Validation loss: 0.5784 (73.4375% accuracy)\nEpoch 200. Training loss: 0.5229 (74.2188% accuracy). Validation loss: 0.4325 (82.8125% accuracy)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test\nmodel.load_state_dict(torch.load('best_naive2.pth'))\n\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n        \n    for i, (x_center, x_real, x, targets) in enumerate(validation_loader):\n        x_center = x_center.to(device)\n        x_real = x_real.to(device)\n        x = x.to(device)\n        targets = torch.flatten(targets).to(device)\n        #targets = targets.reshape(-1, 1).to(device)\n\n        outputs = model(x_center, x_real, x)\n        _, predicted = torch.max(outputs.data, 1)\n\n        n_samples += x.shape[0]\n        n_correct += (predicted == targets).sum().item()\n    \n    acc = float(100 * n_correct / n_samples)\n    print('Test accuracy: {}%'.format(acc))","metadata":{"id":"S-dIIA2EhglE","outputId":"e7e841e3-4b15-4fb0-ad79-0fed74c7cdd2","trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"Test accuracy: 73.125%\n","output_type":"stream"}]},{"cell_type":"code","source":"hit = 0\npositives = 0\nn = 0\nfor i in range(len(test_data)):\n    x_center, real, song, target = test_data[i]\n    \n    target = (target == 1)\n    found = False\n    for c in x_center:\n        if c.tolist() == song.tolist():\n            found = True\n    \n    #if found:\n    #    n += 1\n            \n    if found == target:\n        hit += 1\n\nprint(100.0 * hit / len(test_data))\n#print(hit)","metadata":{"trusted":true},"execution_count":148,"outputs":[{"name":"stdout","text":"94.45\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\ncandidates = {'gmat': [780,750,690,710,780,730,690,720,740,690,610,690,710,680,770,610,580,650,540,590,620,600,550,550,570,670,660,580,650,760,640,620,660,660,680,650,670,580,590,790],\n              'gpa': [4,3.9,3.3,3.7,3.9,3.7,2.3,3.3,3.3,1.7,2.7,3.7,3.7,3.3,3.3,3,2.7,3.7,2.7,2.3,3.3,2,2.3,2.7,3,3.3,3.7,2.3,3.7,3.3,3,2.7,4,3.3,3.3,2.3,2.7,3.3,1.7,3.7],\n              'work_experience': [3,4,3,5,4,6,1,4,5,1,3,5,6,4,3,1,4,6,2,3,2,1,4,1,2,6,4,2,6,5,1,2,4,6,5,1,2,1,4,5],\n              'age': [25,28,24,27,26,31,24,25,28,23,25,27,30,28,26,23,29,31,26,26,25,24,28,23,25,29,28,26,30,30,23,24,27,29,28,22,23,24,28,31],\n              'admitted': [2,2,1,2,2,2,0,2,2,0,0,2,2,1,2,0,0,1,0,0,1,0,0,0,0,1,1,0,1,2,0,0,1,1,1,0,0,0,0,2]\n              }\n\nfor i in len(my_data[0])\n\n\ndf = pd.DataFrame(my_data)\n\nX = df[['gmat', 'gpa','work_experience','age']]\ny = df['admitted']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\n\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsn.heatmap(confusion_matrix, annot=True)\n\nprint('Accuracy: ',metrics.accuracy_score(y_test, y_pred))\nplt.show()\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_data = [item[0] + [item[1]] for item in my_data]\npd.DataFrame(my_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}