{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport os\nimport numpy as np\nimport json","metadata":{"id":"XqOapteeQeH7","trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","metadata":{"id":"HDA0tK6XQh_0","trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"class JsonDataset(data.Dataset):\n    def __init__(self, data_path):\n        f = open(data_path, 'r')\n        self.data = json.loads(f.read())\n        f.close()\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        return torch.FloatTensor(self.data[index][0]), \\\n            torch.FloatTensor(self.data[index][1])","metadata":{"id":"4539zd7OS8u9","trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"train_data = JsonDataset('/kaggle/input/cpsc490/new-multi-rnn-train.json')\nvalidation_data = JsonDataset('/kaggle/input/cpsc490/new-multi-rnn-validation.json')\ntest_data = JsonDataset('/kaggle/input/cpsc490/new-multi-rnn-test.json')","metadata":{"id":"tYtBZxmfVefp","trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"x_seq_size = train_data[0][0].shape[1]\nrnn_num_layers = 2\n\nbatch_size = 64","metadata":{"id":"yHrlQ0mMdIzQ","trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 1, 'pin_memory': True}\ntrain_loader = data.DataLoader(train_data, **params)\nvalidation_loader = data.DataLoader(validation_data, **params)\ntest_loader = data.DataLoader(test_data, **params)","metadata":{"id":"cgF9wQhTdX86","trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self, x_seq_size, rnn_num_layers):\n        super(RNN, self).__init__()\n        \n        self.rnn_num_layers = rnn_num_layers\n\n        self.rnn = nn.LSTM(x_seq_size, x_seq_size, rnn_num_layers,\n                          batch_first = True)\n\n    def forward(self, x_seq):\n        h0 = torch.zeros(self.rnn_num_layers, x_seq.shape[0], x_seq.shape[2]).to(device)\n        c0 = torch.zeros(self.rnn_num_layers, x_seq.shape[0], x_seq.shape[2]).to(device)\n        \n        out, _ = self.rnn(x_seq, (h0, c0))\n        return out[:, -1, :]","metadata":{"id":"ruw1C_2edjo5","trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"def train(model, criterion, optimizer):\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n\n    for i, (x_seq, targets) in enumerate(train_loader):\n        x_seq = x_seq.to(device)\n        targets = targets.to(device)\n\n        outputs = model(x_seq)\n        loss = criterion(outputs, targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total += targets.shape[0]\n        train_loss += loss.item() * targets.shape[0]\n        #_, predicted = outputs.max(1)\n        #correct += predicted.eq(targets).sum().item()\n        \n    epoch_train_loss = train_loss / total\n    #epoch_train_acc = float(100 * correct / total)\n\n    return epoch_train_loss","metadata":{"id":"8_Wqe7PcMMAi","trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"def validation(model, criterion):\n    model.eval()\n    validation_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for i, (x_seq, targets) in enumerate(validation_loader):\n            x_seq = x_seq.to(device)\n            targets = targets.to(device)\n\n            outputs = model(x_seq)\n            loss = criterion(outputs, targets)\n\n        total += targets.shape[0]\n        validation_loss += loss.item() * targets.shape[0]\n        #_, predicted = outputs.max(1)\n        #correct += predicted.eq(targets).sum().item()\n        \n    epoch_validation_loss = validation_loss / total\n    #epoch_validation_acc = float(100 * correct / total)\n\n    return epoch_validation_loss","metadata":{"id":"gxB81t-kMNcl","trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"model = RNN(x_seq_size, rnn_num_layers).to(device)\ncriterion = nn.MSELoss().to(device)\n#criterion = nn.CrossEntropyLoss().to(device)\n#criterion = nn.MSELoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)  \n\nnum_epochs = 1000","metadata":{"id":"6P1JJvbqMO5q","outputId":"12f1a8da-08c8-4eff-c1c7-69e3af3b7d5e","trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# Train\nbest_validation_loss = None\n\nfor epoch in range(0, num_epochs):\n    epoch_train_loss = train(model, criterion, optimizer)\n    epoch_validation_loss = validation(model, criterion)\n    \n    if best_validation_loss == None or epoch_validation_loss < best_validation_loss:\n        torch.save(model.state_dict(), 'best_rnn.pth')\n        print('Saved.')\n        best_validation_loss = epoch_validation_loss\n\n    print('Epoch {}. Training loss: {}. Validation loss: {}.'.format(epoch + 1, \n                                                                format(epoch_train_loss, '.6f'), \n                                                                format(epoch_validation_loss, '.6f')))\n    ","metadata":{"id":"dXp6xbzzMRe_","trusted":true},"execution_count":95,"outputs":[{"name":"stdout","text":"Saved.\nEpoch 1. Training loss: 0.104540. Validation loss: 0.073665.\nSaved.\nEpoch 2. Training loss: 0.059376. Validation loss: 0.042099.\nSaved.\nEpoch 3. Training loss: 0.033428. Validation loss: 0.025508.\nSaved.\nEpoch 4. Training loss: 0.018332. Validation loss: 0.012007.\nEpoch 5. Training loss: 0.011420. Validation loss: 0.012806.\nSaved.\nEpoch 6. Training loss: 0.008176. Validation loss: 0.005974.\nEpoch 7. Training loss: 0.006614. Validation loss: 0.008188.\nSaved.\nEpoch 8. Training loss: 0.005730. Validation loss: 0.002231.\nEpoch 9. Training loss: 0.005160. Validation loss: 0.003986.\nEpoch 10. Training loss: 0.004723. Validation loss: 0.006208.\nEpoch 11. Training loss: 0.004380. Validation loss: 0.004380.\nSaved.\nEpoch 12. Training loss: 0.004100. Validation loss: 0.002048.\nEpoch 13. Training loss: 0.003880. Validation loss: 0.005381.\nSaved.\nEpoch 14. Training loss: 0.003699. Validation loss: 0.001533.\nEpoch 15. Training loss: 0.003525. Validation loss: 0.002999.\nEpoch 16. Training loss: 0.003381. Validation loss: 0.003387.\nEpoch 17. Training loss: 0.003258. Validation loss: 0.002217.\nSaved.\nEpoch 18. Training loss: 0.003147. Validation loss: 0.001062.\nEpoch 19. Training loss: 0.003027. Validation loss: 0.001897.\nEpoch 20. Training loss: 0.002938. Validation loss: 0.001726.\nEpoch 21. Training loss: 0.002844. Validation loss: 0.003069.\nEpoch 22. Training loss: 0.002771. Validation loss: 0.003986.\nEpoch 23. Training loss: 0.002686. Validation loss: 0.003761.\nEpoch 24. Training loss: 0.002629. Validation loss: 0.003466.\nEpoch 25. Training loss: 0.002568. Validation loss: 0.002330.\nEpoch 26. Training loss: 0.002495. Validation loss: 0.003760.\nEpoch 27. Training loss: 0.002434. Validation loss: 0.001772.\nEpoch 28. Training loss: 0.002381. Validation loss: 0.003182.\nEpoch 29. Training loss: 0.002331. Validation loss: 0.002546.\nEpoch 30. Training loss: 0.002280. Validation loss: 0.002030.\nEpoch 31. Training loss: 0.002237. Validation loss: 0.006930.\nEpoch 32. Training loss: 0.002191. Validation loss: 0.001196.\nEpoch 33. Training loss: 0.002146. Validation loss: 0.001130.\nSaved.\nEpoch 34. Training loss: 0.002108. Validation loss: 0.000610.\nEpoch 35. Training loss: 0.002070. Validation loss: 0.001002.\nEpoch 36. Training loss: 0.002032. Validation loss: 0.003569.\nEpoch 37. Training loss: 0.002000. Validation loss: 0.000999.\nSaved.\nEpoch 38. Training loss: 0.001962. Validation loss: 0.000571.\nEpoch 39. Training loss: 0.001935. Validation loss: 0.001220.\nEpoch 40. Training loss: 0.001913. Validation loss: 0.002118.\nEpoch 41. Training loss: 0.001877. Validation loss: 0.001038.\nEpoch 42. Training loss: 0.001857. Validation loss: 0.003724.\nEpoch 43. Training loss: 0.001831. Validation loss: 0.005810.\nEpoch 44. Training loss: 0.001811. Validation loss: 0.002008.\nSaved.\nEpoch 45. Training loss: 0.001785. Validation loss: 0.000539.\nEpoch 46. Training loss: 0.001769. Validation loss: 0.001334.\nEpoch 47. Training loss: 0.001743. Validation loss: 0.001808.\nEpoch 48. Training loss: 0.001727. Validation loss: 0.003430.\nSaved.\nEpoch 49. Training loss: 0.001707. Validation loss: 0.000440.\nEpoch 50. Training loss: 0.001697. Validation loss: 0.000802.\nEpoch 51. Training loss: 0.001673. Validation loss: 0.000559.\nEpoch 52. Training loss: 0.001659. Validation loss: 0.001314.\nEpoch 53. Training loss: 0.001644. Validation loss: 0.001333.\nEpoch 54. Training loss: 0.001636. Validation loss: 0.001289.\nEpoch 55. Training loss: 0.001618. Validation loss: 0.001760.\nEpoch 56. Training loss: 0.001609. Validation loss: 0.000450.\nEpoch 57. Training loss: 0.001592. Validation loss: 0.001701.\nEpoch 58. Training loss: 0.001585. Validation loss: 0.002097.\nEpoch 59. Training loss: 0.001570. Validation loss: 0.001383.\nEpoch 60. Training loss: 0.001560. Validation loss: 0.005097.\nEpoch 61. Training loss: 0.001553. Validation loss: 0.005913.\nEpoch 62. Training loss: 0.001534. Validation loss: 0.002508.\nSaved.\nEpoch 63. Training loss: 0.001532. Validation loss: 0.000283.\nEpoch 64. Training loss: 0.001526. Validation loss: 0.000747.\nEpoch 65. Training loss: 0.001516. Validation loss: 0.003015.\nEpoch 66. Training loss: 0.001506. Validation loss: 0.000478.\nEpoch 67. Training loss: 0.001499. Validation loss: 0.004193.\nEpoch 68. Training loss: 0.001490. Validation loss: 0.004225.\nEpoch 69. Training loss: 0.001485. Validation loss: 0.004151.\nEpoch 70. Training loss: 0.001477. Validation loss: 0.001726.\nEpoch 71. Training loss: 0.001470. Validation loss: 0.002004.\nEpoch 72. Training loss: 0.001463. Validation loss: 0.000473.\nEpoch 73. Training loss: 0.001460. Validation loss: 0.002105.\nEpoch 74. Training loss: 0.001451. Validation loss: 0.001273.\nEpoch 75. Training loss: 0.001445. Validation loss: 0.000906.\nEpoch 76. Training loss: 0.001438. Validation loss: 0.000365.\nEpoch 77. Training loss: 0.001432. Validation loss: 0.001881.\nEpoch 78. Training loss: 0.001429. Validation loss: 0.000780.\nEpoch 79. Training loss: 0.001421. Validation loss: 0.002753.\nEpoch 80. Training loss: 0.001422. Validation loss: 0.000450.\nEpoch 81. Training loss: 0.001412. Validation loss: 0.000552.\nEpoch 82. Training loss: 0.001410. Validation loss: 0.001883.\nEpoch 83. Training loss: 0.001411. Validation loss: 0.003603.\nEpoch 84. Training loss: 0.001400. Validation loss: 0.001997.\nEpoch 85. Training loss: 0.001403. Validation loss: 0.001073.\nEpoch 86. Training loss: 0.001390. Validation loss: 0.000341.\nEpoch 87. Training loss: 0.001388. Validation loss: 0.001723.\nEpoch 88. Training loss: 0.001386. Validation loss: 0.000551.\nEpoch 89. Training loss: 0.001379. Validation loss: 0.001469.\nEpoch 90. Training loss: 0.001375. Validation loss: 0.004257.\nEpoch 91. Training loss: 0.001373. Validation loss: 0.001800.\nEpoch 92. Training loss: 0.001371. Validation loss: 0.002954.\nEpoch 93. Training loss: 0.001373. Validation loss: 0.003537.\nEpoch 94. Training loss: 0.001360. Validation loss: 0.001583.\nEpoch 95. Training loss: 0.001362. Validation loss: 0.002158.\nEpoch 96. Training loss: 0.001361. Validation loss: 0.001395.\nSaved.\nEpoch 97. Training loss: 0.001351. Validation loss: 0.000215.\nEpoch 98. Training loss: 0.001350. Validation loss: 0.000593.\nEpoch 99. Training loss: 0.001350. Validation loss: 0.002655.\nEpoch 100. Training loss: 0.001347. Validation loss: 0.001848.\nEpoch 101. Training loss: 0.001347. Validation loss: 0.002158.\nEpoch 102. Training loss: 0.001343. Validation loss: 0.000849.\nEpoch 103. Training loss: 0.001338. Validation loss: 0.002658.\nEpoch 104. Training loss: 0.001335. Validation loss: 0.001825.\nEpoch 105. Training loss: 0.001336. Validation loss: 0.000681.\nEpoch 106. Training loss: 0.001332. Validation loss: 0.000454.\nEpoch 107. Training loss: 0.001327. Validation loss: 0.000669.\nEpoch 108. Training loss: 0.001326. Validation loss: 0.001620.\nEpoch 109. Training loss: 0.001328. Validation loss: 0.002388.\nEpoch 110. Training loss: 0.001324. Validation loss: 0.002099.\nEpoch 111. Training loss: 0.001318. Validation loss: 0.000225.\nEpoch 112. Training loss: 0.001318. Validation loss: 0.000859.\nEpoch 113. Training loss: 0.001317. Validation loss: 0.000450.\nEpoch 114. Training loss: 0.001315. Validation loss: 0.000481.\nEpoch 115. Training loss: 0.001313. Validation loss: 0.001997.\nEpoch 116. Training loss: 0.001310. Validation loss: 0.002051.\nEpoch 117. Training loss: 0.001312. Validation loss: 0.001670.\nEpoch 118. Training loss: 0.001305. Validation loss: 0.000269.\nEpoch 119. Training loss: 0.001308. Validation loss: 0.000603.\nEpoch 120. Training loss: 0.001305. Validation loss: 0.003612.\nEpoch 121. Training loss: 0.001303. Validation loss: 0.003254.\nEpoch 122. Training loss: 0.001300. Validation loss: 0.000917.\nEpoch 123. Training loss: 0.001297. Validation loss: 0.000890.\nEpoch 124. Training loss: 0.001297. Validation loss: 0.000239.\nEpoch 125. Training loss: 0.001295. Validation loss: 0.003189.\nEpoch 126. Training loss: 0.001293. Validation loss: 0.002096.\nEpoch 127. Training loss: 0.001291. Validation loss: 0.000527.\nEpoch 128. Training loss: 0.001291. Validation loss: 0.002089.\nEpoch 129. Training loss: 0.001287. Validation loss: 0.000713.\nEpoch 130. Training loss: 0.001286. Validation loss: 0.000933.\nEpoch 131. Training loss: 0.001284. Validation loss: 0.000592.\nEpoch 132. Training loss: 0.001287. Validation loss: 0.000492.\nEpoch 133. Training loss: 0.001279. Validation loss: 0.002066.\nEpoch 134. Training loss: 0.001279. Validation loss: 0.000308.\nEpoch 135. Training loss: 0.001281. Validation loss: 0.001607.\nEpoch 136. Training loss: 0.001278. Validation loss: 0.003683.\nEpoch 137. Training loss: 0.001272. Validation loss: 0.000670.\nEpoch 138. Training loss: 0.001275. Validation loss: 0.001680.\nEpoch 139. Training loss: 0.001275. Validation loss: 0.000365.\nEpoch 140. Training loss: 0.001271. Validation loss: 0.003683.\nEpoch 141. Training loss: 0.001278. Validation loss: 0.003681.\nEpoch 142. Training loss: 0.001267. Validation loss: 0.000255.\nEpoch 143. Training loss: 0.001269. Validation loss: 0.000968.\nEpoch 144. Training loss: 0.001269. Validation loss: 0.001910.\nEpoch 145. Training loss: 0.001267. Validation loss: 0.000368.\nEpoch 146. Training loss: 0.001271. Validation loss: 0.001878.\nEpoch 147. Training loss: 0.001263. Validation loss: 0.003936.\nEpoch 148. Training loss: 0.001262. Validation loss: 0.001791.\nEpoch 149. Training loss: 0.001265. Validation loss: 0.000235.\nEpoch 150. Training loss: 0.001265. Validation loss: 0.000815.\nEpoch 151. Training loss: 0.001264. Validation loss: 0.000865.\nEpoch 152. Training loss: 0.001261. Validation loss: 0.001820.\nEpoch 153. Training loss: 0.001261. Validation loss: 0.002368.\nEpoch 154. Training loss: 0.001255. Validation loss: 0.000727.\nEpoch 155. Training loss: 0.001256. Validation loss: 0.001778.\nEpoch 156. Training loss: 0.001258. Validation loss: 0.000805.\nEpoch 157. Training loss: 0.001250. Validation loss: 0.002404.\nEpoch 158. Training loss: 0.001258. Validation loss: 0.002024.\nEpoch 159. Training loss: 0.001254. Validation loss: 0.000313.\nEpoch 160. Training loss: 0.001256. Validation loss: 0.002867.\nEpoch 161. Training loss: 0.001249. Validation loss: 0.003601.\nEpoch 162. Training loss: 0.001252. Validation loss: 0.001103.\nEpoch 163. Training loss: 0.001251. Validation loss: 0.000544.\nEpoch 164. Training loss: 0.001252. Validation loss: 0.000370.\nEpoch 165. Training loss: 0.001251. Validation loss: 0.002374.\nEpoch 166. Training loss: 0.001248. Validation loss: 0.000324.\nEpoch 167. Training loss: 0.001244. Validation loss: 0.005465.\nEpoch 168. Training loss: 0.001248. Validation loss: 0.003617.\nEpoch 169. Training loss: 0.001244. Validation loss: 0.000490.\nEpoch 170. Training loss: 0.001244. Validation loss: 0.001032.\nEpoch 171. Training loss: 0.001242. Validation loss: 0.002644.\nEpoch 172. Training loss: 0.001249. Validation loss: 0.001970.\nEpoch 173. Training loss: 0.001237. Validation loss: 0.000736.\nEpoch 174. Training loss: 0.001244. Validation loss: 0.000625.\nEpoch 175. Training loss: 0.001243. Validation loss: 0.000576.\nEpoch 176. Training loss: 0.001242. Validation loss: 0.001133.\nEpoch 177. Training loss: 0.001238. Validation loss: 0.001056.\nEpoch 178. Training loss: 0.001238. Validation loss: 0.000282.\nEpoch 179. Training loss: 0.001238. Validation loss: 0.003575.\nEpoch 180. Training loss: 0.001240. Validation loss: 0.002388.\nEpoch 181. Training loss: 0.001237. Validation loss: 0.000455.\nEpoch 182. Training loss: 0.001238. Validation loss: 0.003597.\nEpoch 183. Training loss: 0.001236. Validation loss: 0.000230.\nEpoch 184. Training loss: 0.001236. Validation loss: 0.002914.\nEpoch 185. Training loss: 0.001232. Validation loss: 0.002819.\nEpoch 186. Training loss: 0.001233. Validation loss: 0.004316.\nEpoch 187. Training loss: 0.001232. Validation loss: 0.002356.\nEpoch 188. Training loss: 0.001234. Validation loss: 0.002285.\nEpoch 189. Training loss: 0.001231. Validation loss: 0.000294.\nEpoch 190. Training loss: 0.001229. Validation loss: 0.001696.\nEpoch 191. Training loss: 0.001230. Validation loss: 0.002702.\nEpoch 192. Training loss: 0.001235. Validation loss: 0.000662.\nEpoch 193. Training loss: 0.001232. Validation loss: 0.000435.\nEpoch 194. Training loss: 0.001228. Validation loss: 0.001552.\nEpoch 195. Training loss: 0.001228. Validation loss: 0.001251.\nEpoch 196. Training loss: 0.001231. Validation loss: 0.003988.\nEpoch 197. Training loss: 0.001229. Validation loss: 0.000237.\nEpoch 198. Training loss: 0.001229. Validation loss: 0.000767.\nEpoch 199. Training loss: 0.001228. Validation loss: 0.002343.\nEpoch 200. Training loss: 0.001223. Validation loss: 0.002125.\nEpoch 201. Training loss: 0.001221. Validation loss: 0.001769.\nEpoch 202. Training loss: 0.001224. Validation loss: 0.003718.\nEpoch 203. Training loss: 0.001224. Validation loss: 0.002036.\nEpoch 204. Training loss: 0.001223. Validation loss: 0.002279.\nEpoch 205. Training loss: 0.001227. Validation loss: 0.001799.\nEpoch 206. Training loss: 0.001225. Validation loss: 0.001667.\nEpoch 207. Training loss: 0.001225. Validation loss: 0.001940.\nEpoch 208. Training loss: 0.001223. Validation loss: 0.001576.\nEpoch 209. Training loss: 0.001223. Validation loss: 0.000398.\nEpoch 210. Training loss: 0.001223. Validation loss: 0.000286.\nEpoch 211. Training loss: 0.001223. Validation loss: 0.001917.\nEpoch 212. Training loss: 0.001222. Validation loss: 0.001434.\nSaved.\nEpoch 213. Training loss: 0.001219. Validation loss: 0.000184.\nSaved.\nEpoch 214. Training loss: 0.001221. Validation loss: 0.000123.\nEpoch 215. Training loss: 0.001219. Validation loss: 0.002238.\nEpoch 216. Training loss: 0.001221. Validation loss: 0.001879.\nEpoch 217. Training loss: 0.001218. Validation loss: 0.000430.\nEpoch 218. Training loss: 0.001218. Validation loss: 0.000932.\nEpoch 219. Training loss: 0.001221. Validation loss: 0.000633.\nEpoch 220. Training loss: 0.001214. Validation loss: 0.000685.\nEpoch 221. Training loss: 0.001218. Validation loss: 0.001690.\nEpoch 222. Training loss: 0.001217. Validation loss: 0.001136.\nEpoch 223. Training loss: 0.001216. Validation loss: 0.000994.\nEpoch 224. Training loss: 0.001219. Validation loss: 0.001908.\nEpoch 225. Training loss: 0.001213. Validation loss: 0.001722.\nEpoch 226. Training loss: 0.001216. Validation loss: 0.002007.\nEpoch 227. Training loss: 0.001213. Validation loss: 0.001041.\nEpoch 228. Training loss: 0.001215. Validation loss: 0.002580.\nEpoch 229. Training loss: 0.001211. Validation loss: 0.002194.\nEpoch 230. Training loss: 0.001213. Validation loss: 0.003708.\nEpoch 231. Training loss: 0.001212. Validation loss: 0.000527.\nEpoch 232. Training loss: 0.001217. Validation loss: 0.000490.\nEpoch 233. Training loss: 0.001213. Validation loss: 0.001829.\nEpoch 234. Training loss: 0.001215. Validation loss: 0.001506.\nEpoch 235. Training loss: 0.001213. Validation loss: 0.001955.\nEpoch 236. Training loss: 0.001212. Validation loss: 0.002380.\nEpoch 237. Training loss: 0.001214. Validation loss: 0.002150.\nEpoch 238. Training loss: 0.001209. Validation loss: 0.002108.\nEpoch 239. Training loss: 0.001209. Validation loss: 0.000536.\nEpoch 240. Training loss: 0.001211. Validation loss: 0.000262.\nEpoch 241. Training loss: 0.001210. Validation loss: 0.003012.\nEpoch 242. Training loss: 0.001211. Validation loss: 0.000251.\nEpoch 243. Training loss: 0.001210. Validation loss: 0.001996.\nEpoch 244. Training loss: 0.001207. Validation loss: 0.000969.\nEpoch 245. Training loss: 0.001211. Validation loss: 0.000237.\nEpoch 246. Training loss: 0.001208. Validation loss: 0.002324.\nEpoch 247. Training loss: 0.001208. Validation loss: 0.003459.\nEpoch 248. Training loss: 0.001206. Validation loss: 0.001191.\nEpoch 249. Training loss: 0.001209. Validation loss: 0.001605.\nEpoch 250. Training loss: 0.001205. Validation loss: 0.001266.\nEpoch 251. Training loss: 0.001207. Validation loss: 0.001636.\nEpoch 252. Training loss: 0.001205. Validation loss: 0.000438.\nEpoch 253. Training loss: 0.001210. Validation loss: 0.000995.\nEpoch 254. Training loss: 0.001204. Validation loss: 0.002143.\nEpoch 255. Training loss: 0.001206. Validation loss: 0.000723.\nEpoch 256. Training loss: 0.001204. Validation loss: 0.001699.\nEpoch 257. Training loss: 0.001203. Validation loss: 0.000591.\nEpoch 258. Training loss: 0.001205. Validation loss: 0.000632.\nEpoch 259. Training loss: 0.001212. Validation loss: 0.003603.\nEpoch 260. Training loss: 0.001206. Validation loss: 0.000460.\nEpoch 261. Training loss: 0.001206. Validation loss: 0.003143.\nEpoch 262. Training loss: 0.001206. Validation loss: 0.000358.\nEpoch 263. Training loss: 0.001205. Validation loss: 0.001491.\nEpoch 264. Training loss: 0.001203. Validation loss: 0.001040.\nEpoch 265. Training loss: 0.001203. Validation loss: 0.000174.\nEpoch 266. Training loss: 0.001205. Validation loss: 0.000393.\nEpoch 267. Training loss: 0.001204. Validation loss: 0.000292.\nEpoch 268. Training loss: 0.001204. Validation loss: 0.001994.\nEpoch 269. Training loss: 0.001200. Validation loss: 0.000163.\nEpoch 270. Training loss: 0.001202. Validation loss: 0.001074.\nEpoch 271. Training loss: 0.001205. Validation loss: 0.000620.\nEpoch 272. Training loss: 0.001199. Validation loss: 0.003945.\nEpoch 273. Training loss: 0.001204. Validation loss: 0.001108.\nEpoch 274. Training loss: 0.001200. Validation loss: 0.001323.\nEpoch 275. Training loss: 0.001201. Validation loss: 0.003372.\nEpoch 276. Training loss: 0.001202. Validation loss: 0.000188.\nEpoch 277. Training loss: 0.001201. Validation loss: 0.000784.\nEpoch 278. Training loss: 0.001202. Validation loss: 0.002179.\nEpoch 279. Training loss: 0.001196. Validation loss: 0.000770.\nEpoch 280. Training loss: 0.001204. Validation loss: 0.003867.\nEpoch 281. Training loss: 0.001200. Validation loss: 0.002068.\nEpoch 282. Training loss: 0.001203. Validation loss: 0.002190.\nEpoch 283. Training loss: 0.001199. Validation loss: 0.001430.\nEpoch 284. Training loss: 0.001198. Validation loss: 0.003427.\nEpoch 285. Training loss: 0.001200. Validation loss: 0.001970.\nEpoch 286. Training loss: 0.001202. Validation loss: 0.001507.\nEpoch 287. Training loss: 0.001196. Validation loss: 0.000235.\nEpoch 288. Training loss: 0.001198. Validation loss: 0.000363.\nEpoch 289. Training loss: 0.001200. Validation loss: 0.000942.\nEpoch 290. Training loss: 0.001199. Validation loss: 0.004705.\nEpoch 291. Training loss: 0.001199. Validation loss: 0.001227.\nEpoch 292. Training loss: 0.001195. Validation loss: 0.000155.\nEpoch 293. Training loss: 0.001200. Validation loss: 0.000350.\nEpoch 294. Training loss: 0.001199. Validation loss: 0.002433.\nEpoch 295. Training loss: 0.001198. Validation loss: 0.000127.\nEpoch 296. Training loss: 0.001198. Validation loss: 0.000236.\nEpoch 297. Training loss: 0.001198. Validation loss: 0.000167.\nEpoch 298. Training loss: 0.001195. Validation loss: 0.001652.\nEpoch 299. Training loss: 0.001197. Validation loss: 0.002099.\nEpoch 300. Training loss: 0.001196. Validation loss: 0.002085.\nEpoch 301. Training loss: 0.001198. Validation loss: 0.001027.\nEpoch 302. Training loss: 0.001195. Validation loss: 0.000306.\nEpoch 303. Training loss: 0.001193. Validation loss: 0.000196.\nEpoch 304. Training loss: 0.001195. Validation loss: 0.000661.\nEpoch 305. Training loss: 0.001197. Validation loss: 0.001847.\nEpoch 306. Training loss: 0.001192. Validation loss: 0.001916.\nEpoch 307. Training loss: 0.001195. Validation loss: 0.003488.\nEpoch 308. Training loss: 0.001195. Validation loss: 0.002337.\nEpoch 309. Training loss: 0.001194. Validation loss: 0.000171.\nEpoch 310. Training loss: 0.001193. Validation loss: 0.001855.\nEpoch 311. Training loss: 0.001197. Validation loss: 0.000165.\nEpoch 312. Training loss: 0.001194. Validation loss: 0.000547.\nEpoch 313. Training loss: 0.001189. Validation loss: 0.002337.\nEpoch 314. Training loss: 0.001193. Validation loss: 0.000178.\nEpoch 315. Training loss: 0.001194. Validation loss: 0.000686.\nEpoch 316. Training loss: 0.001192. Validation loss: 0.001147.\nEpoch 317. Training loss: 0.001192. Validation loss: 0.002776.\nEpoch 318. Training loss: 0.001194. Validation loss: 0.000161.\nEpoch 319. Training loss: 0.001193. Validation loss: 0.000491.\nEpoch 320. Training loss: 0.001192. Validation loss: 0.001012.\nEpoch 321. Training loss: 0.001194. Validation loss: 0.002755.\nEpoch 322. Training loss: 0.001192. Validation loss: 0.000402.\nEpoch 323. Training loss: 0.001188. Validation loss: 0.003462.\nEpoch 324. Training loss: 0.001194. Validation loss: 0.003805.\nEpoch 325. Training loss: 0.001190. Validation loss: 0.001148.\nEpoch 326. Training loss: 0.001191. Validation loss: 0.000159.\nEpoch 327. Training loss: 0.001189. Validation loss: 0.003375.\nEpoch 328. Training loss: 0.001192. Validation loss: 0.002296.\nEpoch 329. Training loss: 0.001190. Validation loss: 0.003160.\nEpoch 330. Training loss: 0.001194. Validation loss: 0.001761.\nEpoch 331. Training loss: 0.001188. Validation loss: 0.003801.\nEpoch 332. Training loss: 0.001191. Validation loss: 0.001846.\nEpoch 333. Training loss: 0.001191. Validation loss: 0.001631.\nEpoch 334. Training loss: 0.001192. Validation loss: 0.000197.\nEpoch 335. Training loss: 0.001191. Validation loss: 0.003704.\nEpoch 336. Training loss: 0.001188. Validation loss: 0.002330.\nEpoch 337. Training loss: 0.001191. Validation loss: 0.000360.\nEpoch 338. Training loss: 0.001188. Validation loss: 0.001982.\nEpoch 339. Training loss: 0.001190. Validation loss: 0.001416.\nEpoch 340. Training loss: 0.001188. Validation loss: 0.003215.\nEpoch 341. Training loss: 0.001192. Validation loss: 0.001866.\nEpoch 342. Training loss: 0.001188. Validation loss: 0.002210.\nEpoch 343. Training loss: 0.001192. Validation loss: 0.000945.\nEpoch 344. Training loss: 0.001186. Validation loss: 0.001616.\nEpoch 345. Training loss: 0.001189. Validation loss: 0.000323.\nEpoch 346. Training loss: 0.001189. Validation loss: 0.000711.\nEpoch 347. Training loss: 0.001190. Validation loss: 0.003779.\nEpoch 348. Training loss: 0.001189. Validation loss: 0.001787.\nEpoch 349. Training loss: 0.001187. Validation loss: 0.002957.\nEpoch 350. Training loss: 0.001189. Validation loss: 0.000908.\nEpoch 351. Training loss: 0.001189. Validation loss: 0.000180.\nEpoch 352. Training loss: 0.001191. Validation loss: 0.002051.\nEpoch 353. Training loss: 0.001186. Validation loss: 0.004102.\nEpoch 354. Training loss: 0.001185. Validation loss: 0.000464.\nEpoch 355. Training loss: 0.001186. Validation loss: 0.003081.\nEpoch 356. Training loss: 0.001187. Validation loss: 0.001050.\nEpoch 357. Training loss: 0.001188. Validation loss: 0.005469.\nEpoch 358. Training loss: 0.001187. Validation loss: 0.006124.\nEpoch 359. Training loss: 0.001185. Validation loss: 0.000247.\nEpoch 360. Training loss: 0.001191. Validation loss: 0.001796.\nEpoch 361. Training loss: 0.001186. Validation loss: 0.002112.\nEpoch 362. Training loss: 0.001182. Validation loss: 0.000922.\nEpoch 363. Training loss: 0.001187. Validation loss: 0.000497.\nEpoch 364. Training loss: 0.001184. Validation loss: 0.001948.\nEpoch 365. Training loss: 0.001188. Validation loss: 0.000508.\nEpoch 366. Training loss: 0.001185. Validation loss: 0.002328.\nEpoch 367. Training loss: 0.001189. Validation loss: 0.000863.\nEpoch 368. Training loss: 0.001186. Validation loss: 0.000242.\nEpoch 369. Training loss: 0.001185. Validation loss: 0.001038.\nEpoch 370. Training loss: 0.001186. Validation loss: 0.003587.\nEpoch 371. Training loss: 0.001185. Validation loss: 0.002398.\nEpoch 372. Training loss: 0.001186. Validation loss: 0.000171.\nEpoch 373. Training loss: 0.001185. Validation loss: 0.001445.\nEpoch 374. Training loss: 0.001183. Validation loss: 0.000334.\nEpoch 375. Training loss: 0.001182. Validation loss: 0.000614.\nEpoch 376. Training loss: 0.001182. Validation loss: 0.000296.\nEpoch 377. Training loss: 0.001189. Validation loss: 0.000264.\nEpoch 378. Training loss: 0.001184. Validation loss: 0.000442.\nEpoch 379. Training loss: 0.001183. Validation loss: 0.001049.\nEpoch 380. Training loss: 0.001186. Validation loss: 0.000185.\nEpoch 381. Training loss: 0.001182. Validation loss: 0.000389.\nEpoch 382. Training loss: 0.001184. Validation loss: 0.001545.\nEpoch 383. Training loss: 0.001186. Validation loss: 0.001140.\nEpoch 384. Training loss: 0.001182. Validation loss: 0.002312.\nEpoch 385. Training loss: 0.001186. Validation loss: 0.000997.\nEpoch 386. Training loss: 0.001182. Validation loss: 0.001077.\nEpoch 387. Training loss: 0.001183. Validation loss: 0.001602.\nEpoch 388. Training loss: 0.001183. Validation loss: 0.003636.\nEpoch 389. Training loss: 0.001181. Validation loss: 0.002956.\nEpoch 390. Training loss: 0.001187. Validation loss: 0.001912.\nEpoch 391. Training loss: 0.001184. Validation loss: 0.001897.\nEpoch 392. Training loss: 0.001188. Validation loss: 0.000566.\nEpoch 393. Training loss: 0.001185. Validation loss: 0.002781.\nEpoch 394. Training loss: 0.001185. Validation loss: 0.001658.\nEpoch 395. Training loss: 0.001185. Validation loss: 0.002875.\nEpoch 396. Training loss: 0.001187. Validation loss: 0.002862.\nEpoch 397. Training loss: 0.001184. Validation loss: 0.001208.\nEpoch 398. Training loss: 0.001180. Validation loss: 0.003552.\nEpoch 399. Training loss: 0.001183. Validation loss: 0.000352.\nEpoch 400. Training loss: 0.001184. Validation loss: 0.001819.\nEpoch 401. Training loss: 0.001181. Validation loss: 0.001626.\nEpoch 402. Training loss: 0.001180. Validation loss: 0.000155.\nEpoch 403. Training loss: 0.001186. Validation loss: 0.001522.\nEpoch 404. Training loss: 0.001182. Validation loss: 0.002029.\nEpoch 405. Training loss: 0.001184. Validation loss: 0.002177.\nEpoch 406. Training loss: 0.001186. Validation loss: 0.000175.\nEpoch 407. Training loss: 0.001182. Validation loss: 0.001044.\nEpoch 408. Training loss: 0.001179. Validation loss: 0.003655.\nEpoch 409. Training loss: 0.001188. Validation loss: 0.000909.\nEpoch 410. Training loss: 0.001184. Validation loss: 0.001740.\nEpoch 411. Training loss: 0.001180. Validation loss: 0.001483.\nEpoch 412. Training loss: 0.001184. Validation loss: 0.000331.\nEpoch 413. Training loss: 0.001180. Validation loss: 0.001535.\nEpoch 414. Training loss: 0.001181. Validation loss: 0.001984.\nEpoch 415. Training loss: 0.001182. Validation loss: 0.002004.\nEpoch 416. Training loss: 0.001180. Validation loss: 0.001877.\nEpoch 417. Training loss: 0.001180. Validation loss: 0.003145.\nEpoch 418. Training loss: 0.001180. Validation loss: 0.002837.\nEpoch 419. Training loss: 0.001183. Validation loss: 0.001925.\nEpoch 420. Training loss: 0.001184. Validation loss: 0.000468.\nEpoch 421. Training loss: 0.001178. Validation loss: 0.002872.\nEpoch 422. Training loss: 0.001181. Validation loss: 0.001821.\nEpoch 423. Training loss: 0.001180. Validation loss: 0.001696.\nEpoch 424. Training loss: 0.001180. Validation loss: 0.001509.\nEpoch 425. Training loss: 0.001177. Validation loss: 0.002136.\nEpoch 426. Training loss: 0.001181. Validation loss: 0.002538.\nEpoch 427. Training loss: 0.001179. Validation loss: 0.002774.\nEpoch 428. Training loss: 0.001180. Validation loss: 0.001689.\nEpoch 429. Training loss: 0.001180. Validation loss: 0.000159.\nEpoch 430. Training loss: 0.001181. Validation loss: 0.001706.\nEpoch 431. Training loss: 0.001178. Validation loss: 0.002913.\nEpoch 432. Training loss: 0.001181. Validation loss: 0.003705.\nEpoch 433. Training loss: 0.001180. Validation loss: 0.000340.\nEpoch 434. Training loss: 0.001180. Validation loss: 0.003400.\nEpoch 435. Training loss: 0.001180. Validation loss: 0.003097.\nEpoch 436. Training loss: 0.001175. Validation loss: 0.000256.\nEpoch 437. Training loss: 0.001179. Validation loss: 0.001746.\nEpoch 438. Training loss: 0.001177. Validation loss: 0.000213.\nEpoch 439. Training loss: 0.001177. Validation loss: 0.001734.\nEpoch 440. Training loss: 0.001178. Validation loss: 0.000131.\nEpoch 441. Training loss: 0.001176. Validation loss: 0.001380.\nEpoch 442. Training loss: 0.001179. Validation loss: 0.001671.\nEpoch 443. Training loss: 0.001179. Validation loss: 0.000802.\nEpoch 444. Training loss: 0.001181. Validation loss: 0.000802.\nEpoch 445. Training loss: 0.001179. Validation loss: 0.000295.\nEpoch 446. Training loss: 0.001180. Validation loss: 0.003055.\nEpoch 447. Training loss: 0.001181. Validation loss: 0.000251.\nEpoch 448. Training loss: 0.001179. Validation loss: 0.001934.\nEpoch 449. Training loss: 0.001175. Validation loss: 0.004474.\nEpoch 450. Training loss: 0.001179. Validation loss: 0.000189.\nEpoch 451. Training loss: 0.001179. Validation loss: 0.000728.\nEpoch 452. Training loss: 0.001177. Validation loss: 0.002033.\nSaved.\nEpoch 453. Training loss: 0.001179. Validation loss: 0.000115.\nEpoch 454. Training loss: 0.001177. Validation loss: 0.001121.\nEpoch 455. Training loss: 0.001177. Validation loss: 0.002533.\nEpoch 456. Training loss: 0.001176. Validation loss: 0.003108.\nEpoch 457. Training loss: 0.001175. Validation loss: 0.000388.\nEpoch 458. Training loss: 0.001175. Validation loss: 0.000231.\nEpoch 459. Training loss: 0.001176. Validation loss: 0.002379.\nEpoch 460. Training loss: 0.001174. Validation loss: 0.003172.\nEpoch 461. Training loss: 0.001178. Validation loss: 0.004022.\nEpoch 462. Training loss: 0.001179. Validation loss: 0.002462.\nEpoch 463. Training loss: 0.001174. Validation loss: 0.001977.\nEpoch 464. Training loss: 0.001178. Validation loss: 0.003183.\nEpoch 465. Training loss: 0.001176. Validation loss: 0.000664.\nEpoch 466. Training loss: 0.001178. Validation loss: 0.001562.\nEpoch 467. Training loss: 0.001176. Validation loss: 0.003044.\nEpoch 468. Training loss: 0.001178. Validation loss: 0.000277.\nEpoch 469. Training loss: 0.001178. Validation loss: 0.001912.\nEpoch 470. Training loss: 0.001175. Validation loss: 0.001403.\nEpoch 471. Training loss: 0.001175. Validation loss: 0.000334.\nEpoch 472. Training loss: 0.001176. Validation loss: 0.001731.\nEpoch 473. Training loss: 0.001177. Validation loss: 0.003604.\nEpoch 474. Training loss: 0.001179. Validation loss: 0.001846.\nEpoch 475. Training loss: 0.001178. Validation loss: 0.001721.\nEpoch 476. Training loss: 0.001177. Validation loss: 0.000536.\nEpoch 477. Training loss: 0.001175. Validation loss: 0.002017.\nEpoch 478. Training loss: 0.001175. Validation loss: 0.001751.\nEpoch 479. Training loss: 0.001176. Validation loss: 0.002407.\nEpoch 480. Training loss: 0.001176. Validation loss: 0.000308.\nEpoch 481. Training loss: 0.001172. Validation loss: 0.002610.\nEpoch 482. Training loss: 0.001175. Validation loss: 0.002752.\nEpoch 483. Training loss: 0.001174. Validation loss: 0.001768.\nEpoch 484. Training loss: 0.001177. Validation loss: 0.002316.\nEpoch 485. Training loss: 0.001174. Validation loss: 0.000175.\nEpoch 486. Training loss: 0.001174. Validation loss: 0.001839.\nEpoch 487. Training loss: 0.001177. Validation loss: 0.005796.\nEpoch 488. Training loss: 0.001174. Validation loss: 0.000598.\nEpoch 489. Training loss: 0.001174. Validation loss: 0.000549.\nEpoch 490. Training loss: 0.001173. Validation loss: 0.002251.\nEpoch 491. Training loss: 0.001176. Validation loss: 0.002711.\nEpoch 492. Training loss: 0.001173. Validation loss: 0.001603.\nEpoch 493. Training loss: 0.001174. Validation loss: 0.002717.\nEpoch 494. Training loss: 0.001171. Validation loss: 0.003740.\nEpoch 495. Training loss: 0.001176. Validation loss: 0.001704.\nEpoch 496. Training loss: 0.001177. Validation loss: 0.001745.\nEpoch 497. Training loss: 0.001173. Validation loss: 0.001532.\nEpoch 498. Training loss: 0.001176. Validation loss: 0.000296.\nEpoch 499. Training loss: 0.001172. Validation loss: 0.000902.\nSaved.\nEpoch 500. Training loss: 0.001175. Validation loss: 0.000099.\nEpoch 501. Training loss: 0.001171. Validation loss: 0.003746.\nEpoch 502. Training loss: 0.001173. Validation loss: 0.000282.\nEpoch 503. Training loss: 0.001175. Validation loss: 0.002195.\nEpoch 504. Training loss: 0.001178. Validation loss: 0.000808.\nEpoch 505. Training loss: 0.001174. Validation loss: 0.001583.\nEpoch 506. Training loss: 0.001173. Validation loss: 0.001979.\nEpoch 507. Training loss: 0.001176. Validation loss: 0.000962.\nEpoch 508. Training loss: 0.001172. Validation loss: 0.001195.\nEpoch 509. Training loss: 0.001174. Validation loss: 0.000446.\nEpoch 510. Training loss: 0.001170. Validation loss: 0.001502.\nEpoch 511. Training loss: 0.001176. Validation loss: 0.002410.\nEpoch 512. Training loss: 0.001172. Validation loss: 0.002000.\nEpoch 513. Training loss: 0.001176. Validation loss: 0.000644.\nEpoch 514. Training loss: 0.001178. Validation loss: 0.000494.\nEpoch 515. Training loss: 0.001173. Validation loss: 0.001862.\nEpoch 516. Training loss: 0.001177. Validation loss: 0.001813.\nEpoch 517. Training loss: 0.001174. Validation loss: 0.001052.\nEpoch 518. Training loss: 0.001171. Validation loss: 0.000230.\nEpoch 519. Training loss: 0.001173. Validation loss: 0.000578.\nEpoch 520. Training loss: 0.001172. Validation loss: 0.002877.\nEpoch 521. Training loss: 0.001169. Validation loss: 0.000358.\nEpoch 522. Training loss: 0.001175. Validation loss: 0.000248.\nEpoch 523. Training loss: 0.001171. Validation loss: 0.001584.\nEpoch 524. Training loss: 0.001172. Validation loss: 0.002349.\nEpoch 525. Training loss: 0.001172. Validation loss: 0.000323.\nEpoch 526. Training loss: 0.001172. Validation loss: 0.000280.\nEpoch 527. Training loss: 0.001172. Validation loss: 0.000312.\nEpoch 528. Training loss: 0.001172. Validation loss: 0.000457.\nEpoch 529. Training loss: 0.001173. Validation loss: 0.001556.\nEpoch 530. Training loss: 0.001172. Validation loss: 0.004033.\nEpoch 531. Training loss: 0.001170. Validation loss: 0.000726.\nEpoch 532. Training loss: 0.001171. Validation loss: 0.005286.\nEpoch 533. Training loss: 0.001172. Validation loss: 0.000558.\nEpoch 534. Training loss: 0.001176. Validation loss: 0.002277.\nEpoch 535. Training loss: 0.001171. Validation loss: 0.000612.\nEpoch 536. Training loss: 0.001171. Validation loss: 0.001847.\nEpoch 537. Training loss: 0.001170. Validation loss: 0.000406.\nEpoch 538. Training loss: 0.001174. Validation loss: 0.001733.\nEpoch 539. Training loss: 0.001172. Validation loss: 0.000149.\nEpoch 540. Training loss: 0.001172. Validation loss: 0.001162.\nEpoch 541. Training loss: 0.001171. Validation loss: 0.002481.\nEpoch 542. Training loss: 0.001170. Validation loss: 0.001004.\nEpoch 543. Training loss: 0.001169. Validation loss: 0.001315.\nEpoch 544. Training loss: 0.001168. Validation loss: 0.000921.\nEpoch 545. Training loss: 0.001173. Validation loss: 0.001078.\nEpoch 546. Training loss: 0.001171. Validation loss: 0.000167.\nEpoch 547. Training loss: 0.001172. Validation loss: 0.000311.\nEpoch 548. Training loss: 0.001173. Validation loss: 0.001274.\nEpoch 549. Training loss: 0.001171. Validation loss: 0.003044.\nEpoch 550. Training loss: 0.001169. Validation loss: 0.000534.\nEpoch 551. Training loss: 0.001172. Validation loss: 0.003319.\nEpoch 552. Training loss: 0.001172. Validation loss: 0.000363.\nEpoch 553. Training loss: 0.001169. Validation loss: 0.000199.\nEpoch 554. Training loss: 0.001168. Validation loss: 0.001659.\nEpoch 555. Training loss: 0.001168. Validation loss: 0.000100.\nEpoch 556. Training loss: 0.001171. Validation loss: 0.000388.\nEpoch 557. Training loss: 0.001170. Validation loss: 0.000389.\nEpoch 558. Training loss: 0.001172. Validation loss: 0.001748.\nEpoch 559. Training loss: 0.001173. Validation loss: 0.002030.\nEpoch 560. Training loss: 0.001170. Validation loss: 0.003194.\nSaved.\nEpoch 561. Training loss: 0.001169. Validation loss: 0.000083.\nEpoch 562. Training loss: 0.001170. Validation loss: 0.000387.\nEpoch 563. Training loss: 0.001170. Validation loss: 0.001712.\nEpoch 564. Training loss: 0.001172. Validation loss: 0.002017.\nEpoch 565. Training loss: 0.001168. Validation loss: 0.001368.\nEpoch 566. Training loss: 0.001169. Validation loss: 0.003667.\nEpoch 567. Training loss: 0.001166. Validation loss: 0.000656.\nEpoch 568. Training loss: 0.001168. Validation loss: 0.000442.\nEpoch 569. Training loss: 0.001167. Validation loss: 0.001806.\nEpoch 570. Training loss: 0.001171. Validation loss: 0.004523.\nEpoch 571. Training loss: 0.001173. Validation loss: 0.001834.\nEpoch 572. Training loss: 0.001169. Validation loss: 0.000555.\nEpoch 573. Training loss: 0.001172. Validation loss: 0.002039.\nEpoch 574. Training loss: 0.001175. Validation loss: 0.000415.\nEpoch 575. Training loss: 0.001171. Validation loss: 0.003133.\nEpoch 576. Training loss: 0.001169. Validation loss: 0.000429.\nEpoch 577. Training loss: 0.001167. Validation loss: 0.001120.\nEpoch 578. Training loss: 0.001169. Validation loss: 0.001863.\nEpoch 579. Training loss: 0.001173. Validation loss: 0.001972.\nEpoch 580. Training loss: 0.001167. Validation loss: 0.000818.\nEpoch 581. Training loss: 0.001165. Validation loss: 0.001701.\nEpoch 582. Training loss: 0.001170. Validation loss: 0.000445.\nEpoch 583. Training loss: 0.001168. Validation loss: 0.000531.\nEpoch 584. Training loss: 0.001169. Validation loss: 0.001946.\nEpoch 585. Training loss: 0.001168. Validation loss: 0.000327.\nEpoch 586. Training loss: 0.001169. Validation loss: 0.000134.\nEpoch 587. Training loss: 0.001165. Validation loss: 0.004172.\nEpoch 588. Training loss: 0.001165. Validation loss: 0.001475.\nEpoch 589. Training loss: 0.001169. Validation loss: 0.000382.\nEpoch 590. Training loss: 0.001168. Validation loss: 0.001719.\nEpoch 591. Training loss: 0.001167. Validation loss: 0.001988.\nEpoch 592. Training loss: 0.001167. Validation loss: 0.002654.\nEpoch 593. Training loss: 0.001168. Validation loss: 0.001749.\nEpoch 594. Training loss: 0.001168. Validation loss: 0.000814.\nEpoch 595. Training loss: 0.001169. Validation loss: 0.003668.\nEpoch 596. Training loss: 0.001169. Validation loss: 0.001280.\nEpoch 597. Training loss: 0.001171. Validation loss: 0.000504.\nEpoch 598. Training loss: 0.001168. Validation loss: 0.002441.\nEpoch 599. Training loss: 0.001170. Validation loss: 0.000337.\nEpoch 600. Training loss: 0.001168. Validation loss: 0.000379.\nEpoch 601. Training loss: 0.001167. Validation loss: 0.002104.\nEpoch 602. Training loss: 0.001167. Validation loss: 0.001913.\nEpoch 603. Training loss: 0.001168. Validation loss: 0.000779.\nEpoch 604. Training loss: 0.001169. Validation loss: 0.001789.\nEpoch 605. Training loss: 0.001165. Validation loss: 0.000263.\nEpoch 606. Training loss: 0.001168. Validation loss: 0.000322.\nEpoch 607. Training loss: 0.001166. Validation loss: 0.001954.\nEpoch 608. Training loss: 0.001167. Validation loss: 0.006231.\nEpoch 609. Training loss: 0.001169. Validation loss: 0.000365.\nEpoch 610. Training loss: 0.001169. Validation loss: 0.001162.\nEpoch 611. Training loss: 0.001165. Validation loss: 0.002055.\nEpoch 612. Training loss: 0.001165. Validation loss: 0.000214.\nEpoch 613. Training loss: 0.001166. Validation loss: 0.001018.\nEpoch 614. Training loss: 0.001167. Validation loss: 0.000686.\nEpoch 615. Training loss: 0.001168. Validation loss: 0.000829.\nEpoch 616. Training loss: 0.001165. Validation loss: 0.002878.\nEpoch 617. Training loss: 0.001166. Validation loss: 0.003680.\nEpoch 618. Training loss: 0.001168. Validation loss: 0.003024.\nEpoch 619. Training loss: 0.001163. Validation loss: 0.000399.\nEpoch 620. Training loss: 0.001166. Validation loss: 0.000696.\nEpoch 621. Training loss: 0.001165. Validation loss: 0.000232.\nEpoch 622. Training loss: 0.001166. Validation loss: 0.002841.\nEpoch 623. Training loss: 0.001166. Validation loss: 0.001660.\nEpoch 624. Training loss: 0.001168. Validation loss: 0.002654.\nEpoch 625. Training loss: 0.001168. Validation loss: 0.001976.\nEpoch 626. Training loss: 0.001169. Validation loss: 0.001539.\nEpoch 627. Training loss: 0.001165. Validation loss: 0.000519.\nEpoch 628. Training loss: 0.001165. Validation loss: 0.001498.\nEpoch 629. Training loss: 0.001167. Validation loss: 0.002589.\nEpoch 630. Training loss: 0.001166. Validation loss: 0.000577.\nEpoch 631. Training loss: 0.001164. Validation loss: 0.000546.\nEpoch 632. Training loss: 0.001167. Validation loss: 0.000563.\nEpoch 633. Training loss: 0.001167. Validation loss: 0.001525.\nEpoch 634. Training loss: 0.001170. Validation loss: 0.001860.\nEpoch 635. Training loss: 0.001166. Validation loss: 0.002323.\nEpoch 636. Training loss: 0.001165. Validation loss: 0.000622.\nEpoch 637. Training loss: 0.001169. Validation loss: 0.000730.\nEpoch 638. Training loss: 0.001167. Validation loss: 0.000480.\nEpoch 639. Training loss: 0.001168. Validation loss: 0.000220.\nEpoch 640. Training loss: 0.001167. Validation loss: 0.006469.\nEpoch 641. Training loss: 0.001165. Validation loss: 0.001813.\nEpoch 642. Training loss: 0.001167. Validation loss: 0.000997.\nEpoch 643. Training loss: 0.001165. Validation loss: 0.000170.\nEpoch 644. Training loss: 0.001170. Validation loss: 0.000839.\nEpoch 645. Training loss: 0.001165. Validation loss: 0.004555.\nEpoch 646. Training loss: 0.001165. Validation loss: 0.000290.\nEpoch 647. Training loss: 0.001168. Validation loss: 0.001676.\nEpoch 648. Training loss: 0.001170. Validation loss: 0.000322.\nEpoch 649. Training loss: 0.001165. Validation loss: 0.000113.\nEpoch 650. Training loss: 0.001167. Validation loss: 0.000405.\nEpoch 651. Training loss: 0.001165. Validation loss: 0.000131.\nEpoch 652. Training loss: 0.001164. Validation loss: 0.000619.\nEpoch 653. Training loss: 0.001164. Validation loss: 0.003026.\nEpoch 654. Training loss: 0.001167. Validation loss: 0.002428.\nEpoch 655. Training loss: 0.001165. Validation loss: 0.001201.\nEpoch 656. Training loss: 0.001164. Validation loss: 0.003290.\nEpoch 657. Training loss: 0.001165. Validation loss: 0.003035.\nEpoch 658. Training loss: 0.001168. Validation loss: 0.001558.\nEpoch 659. Training loss: 0.001166. Validation loss: 0.003958.\nEpoch 660. Training loss: 0.001164. Validation loss: 0.000181.\nEpoch 661. Training loss: 0.001164. Validation loss: 0.005030.\nEpoch 662. Training loss: 0.001167. Validation loss: 0.000490.\nEpoch 663. Training loss: 0.001164. Validation loss: 0.000456.\nEpoch 664. Training loss: 0.001167. Validation loss: 0.000954.\nEpoch 665. Training loss: 0.001168. Validation loss: 0.002688.\nEpoch 666. Training loss: 0.001165. Validation loss: 0.003703.\nEpoch 667. Training loss: 0.001165. Validation loss: 0.002440.\nEpoch 668. Training loss: 0.001168. Validation loss: 0.000236.\nEpoch 669. Training loss: 0.001165. Validation loss: 0.004398.\nEpoch 670. Training loss: 0.001163. Validation loss: 0.003194.\nEpoch 671. Training loss: 0.001163. Validation loss: 0.002443.\nEpoch 672. Training loss: 0.001164. Validation loss: 0.001479.\nEpoch 673. Training loss: 0.001162. Validation loss: 0.002403.\nEpoch 674. Training loss: 0.001163. Validation loss: 0.000396.\nEpoch 675. Training loss: 0.001167. Validation loss: 0.002228.\nEpoch 676. Training loss: 0.001166. Validation loss: 0.000666.\nEpoch 677. Training loss: 0.001165. Validation loss: 0.001323.\nEpoch 678. Training loss: 0.001163. Validation loss: 0.000927.\nEpoch 679. Training loss: 0.001165. Validation loss: 0.002127.\nEpoch 680. Training loss: 0.001168. Validation loss: 0.005522.\nEpoch 681. Training loss: 0.001163. Validation loss: 0.000370.\nEpoch 682. Training loss: 0.001160. Validation loss: 0.000258.\nEpoch 683. Training loss: 0.001165. Validation loss: 0.003245.\nEpoch 684. Training loss: 0.001162. Validation loss: 0.000341.\nEpoch 685. Training loss: 0.001163. Validation loss: 0.002591.\nEpoch 686. Training loss: 0.001164. Validation loss: 0.001652.\nEpoch 687. Training loss: 0.001163. Validation loss: 0.000480.\nEpoch 688. Training loss: 0.001165. Validation loss: 0.000489.\nEpoch 689. Training loss: 0.001165. Validation loss: 0.001595.\nEpoch 690. Training loss: 0.001166. Validation loss: 0.001773.\nEpoch 691. Training loss: 0.001161. Validation loss: 0.001926.\nEpoch 692. Training loss: 0.001166. Validation loss: 0.001586.\nEpoch 693. Training loss: 0.001163. Validation loss: 0.000461.\nEpoch 694. Training loss: 0.001159. Validation loss: 0.001079.\nEpoch 695. Training loss: 0.001166. Validation loss: 0.002651.\nEpoch 696. Training loss: 0.001163. Validation loss: 0.001006.\nEpoch 697. Training loss: 0.001163. Validation loss: 0.000318.\nEpoch 698. Training loss: 0.001162. Validation loss: 0.000230.\nEpoch 699. Training loss: 0.001163. Validation loss: 0.000630.\nEpoch 700. Training loss: 0.001164. Validation loss: 0.000574.\nEpoch 701. Training loss: 0.001161. Validation loss: 0.002245.\nEpoch 702. Training loss: 0.001167. Validation loss: 0.000126.\nEpoch 703. Training loss: 0.001160. Validation loss: 0.001527.\nEpoch 704. Training loss: 0.001161. Validation loss: 0.001736.\nEpoch 705. Training loss: 0.001162. Validation loss: 0.002863.\nEpoch 706. Training loss: 0.001165. Validation loss: 0.001491.\nEpoch 707. Training loss: 0.001162. Validation loss: 0.002370.\nEpoch 708. Training loss: 0.001162. Validation loss: 0.001202.\nEpoch 709. Training loss: 0.001162. Validation loss: 0.003156.\nEpoch 710. Training loss: 0.001162. Validation loss: 0.001689.\nEpoch 711. Training loss: 0.001165. Validation loss: 0.000375.\nEpoch 712. Training loss: 0.001163. Validation loss: 0.003102.\nEpoch 713. Training loss: 0.001161. Validation loss: 0.000562.\nEpoch 714. Training loss: 0.001161. Validation loss: 0.000258.\nEpoch 715. Training loss: 0.001161. Validation loss: 0.001625.\nEpoch 716. Training loss: 0.001163. Validation loss: 0.000286.\nEpoch 717. Training loss: 0.001164. Validation loss: 0.003959.\nEpoch 718. Training loss: 0.001161. Validation loss: 0.000348.\nEpoch 719. Training loss: 0.001165. Validation loss: 0.000566.\nEpoch 720. Training loss: 0.001160. Validation loss: 0.001424.\nEpoch 721. Training loss: 0.001166. Validation loss: 0.000471.\nEpoch 722. Training loss: 0.001162. Validation loss: 0.000289.\nEpoch 723. Training loss: 0.001164. Validation loss: 0.001290.\nEpoch 724. Training loss: 0.001161. Validation loss: 0.000403.\nEpoch 725. Training loss: 0.001160. Validation loss: 0.000163.\nEpoch 726. Training loss: 0.001164. Validation loss: 0.000363.\nEpoch 727. Training loss: 0.001164. Validation loss: 0.002152.\nEpoch 728. Training loss: 0.001162. Validation loss: 0.001664.\nEpoch 729. Training loss: 0.001164. Validation loss: 0.001734.\nEpoch 730. Training loss: 0.001161. Validation loss: 0.000950.\nEpoch 731. Training loss: 0.001160. Validation loss: 0.002299.\nEpoch 732. Training loss: 0.001161. Validation loss: 0.000526.\nEpoch 733. Training loss: 0.001161. Validation loss: 0.002362.\nEpoch 734. Training loss: 0.001160. Validation loss: 0.000597.\nEpoch 735. Training loss: 0.001160. Validation loss: 0.001913.\nEpoch 736. Training loss: 0.001162. Validation loss: 0.000456.\nEpoch 737. Training loss: 0.001159. Validation loss: 0.004366.\nEpoch 738. Training loss: 0.001161. Validation loss: 0.001718.\nEpoch 739. Training loss: 0.001162. Validation loss: 0.002817.\nEpoch 740. Training loss: 0.001162. Validation loss: 0.001177.\nEpoch 741. Training loss: 0.001160. Validation loss: 0.001336.\nEpoch 742. Training loss: 0.001162. Validation loss: 0.001714.\nEpoch 743. Training loss: 0.001161. Validation loss: 0.004034.\nEpoch 744. Training loss: 0.001162. Validation loss: 0.000246.\nEpoch 745. Training loss: 0.001162. Validation loss: 0.001970.\nEpoch 746. Training loss: 0.001159. Validation loss: 0.001412.\nEpoch 747. Training loss: 0.001162. Validation loss: 0.003193.\nEpoch 748. Training loss: 0.001163. Validation loss: 0.000439.\nEpoch 749. Training loss: 0.001160. Validation loss: 0.001590.\nEpoch 750. Training loss: 0.001162. Validation loss: 0.001717.\nEpoch 751. Training loss: 0.001160. Validation loss: 0.001949.\nEpoch 752. Training loss: 0.001163. Validation loss: 0.004461.\nEpoch 753. Training loss: 0.001159. Validation loss: 0.001163.\nEpoch 754. Training loss: 0.001157. Validation loss: 0.001726.\nEpoch 755. Training loss: 0.001165. Validation loss: 0.000981.\nEpoch 756. Training loss: 0.001158. Validation loss: 0.001140.\nEpoch 757. Training loss: 0.001162. Validation loss: 0.004689.\nEpoch 758. Training loss: 0.001162. Validation loss: 0.001514.\nEpoch 759. Training loss: 0.001160. Validation loss: 0.001175.\nEpoch 760. Training loss: 0.001159. Validation loss: 0.001439.\nEpoch 761. Training loss: 0.001157. Validation loss: 0.003350.\nEpoch 762. Training loss: 0.001162. Validation loss: 0.002344.\nEpoch 763. Training loss: 0.001160. Validation loss: 0.000978.\nEpoch 764. Training loss: 0.001159. Validation loss: 0.000333.\nEpoch 765. Training loss: 0.001160. Validation loss: 0.002826.\nEpoch 766. Training loss: 0.001162. Validation loss: 0.001359.\nEpoch 767. Training loss: 0.001162. Validation loss: 0.001659.\nEpoch 768. Training loss: 0.001161. Validation loss: 0.002159.\nEpoch 769. Training loss: 0.001160. Validation loss: 0.000226.\nEpoch 770. Training loss: 0.001161. Validation loss: 0.001433.\nEpoch 771. Training loss: 0.001159. Validation loss: 0.004260.\nEpoch 772. Training loss: 0.001158. Validation loss: 0.001010.\nEpoch 773. Training loss: 0.001158. Validation loss: 0.001408.\nEpoch 774. Training loss: 0.001161. Validation loss: 0.002817.\nEpoch 775. Training loss: 0.001161. Validation loss: 0.000389.\nEpoch 776. Training loss: 0.001162. Validation loss: 0.001689.\nEpoch 777. Training loss: 0.001158. Validation loss: 0.002146.\nEpoch 778. Training loss: 0.001158. Validation loss: 0.001947.\nEpoch 779. Training loss: 0.001161. Validation loss: 0.000714.\nEpoch 780. Training loss: 0.001157. Validation loss: 0.000211.\nEpoch 781. Training loss: 0.001161. Validation loss: 0.001906.\nEpoch 782. Training loss: 0.001162. Validation loss: 0.002014.\nEpoch 783. Training loss: 0.001160. Validation loss: 0.001809.\nEpoch 784. Training loss: 0.001160. Validation loss: 0.001279.\nEpoch 785. Training loss: 0.001157. Validation loss: 0.002145.\nEpoch 786. Training loss: 0.001161. Validation loss: 0.000781.\nEpoch 787. Training loss: 0.001160. Validation loss: 0.000288.\nEpoch 788. Training loss: 0.001159. Validation loss: 0.000195.\nEpoch 789. Training loss: 0.001159. Validation loss: 0.002161.\nEpoch 790. Training loss: 0.001160. Validation loss: 0.000463.\nEpoch 791. Training loss: 0.001159. Validation loss: 0.000488.\nEpoch 792. Training loss: 0.001158. Validation loss: 0.000960.\nEpoch 793. Training loss: 0.001161. Validation loss: 0.001179.\nEpoch 794. Training loss: 0.001157. Validation loss: 0.000240.\nEpoch 795. Training loss: 0.001158. Validation loss: 0.000505.\nEpoch 796. Training loss: 0.001157. Validation loss: 0.000227.\nEpoch 797. Training loss: 0.001161. Validation loss: 0.000477.\nEpoch 798. Training loss: 0.001160. Validation loss: 0.003391.\nEpoch 799. Training loss: 0.001156. Validation loss: 0.000692.\nEpoch 800. Training loss: 0.001159. Validation loss: 0.002738.\nEpoch 801. Training loss: 0.001160. Validation loss: 0.001882.\nEpoch 802. Training loss: 0.001158. Validation loss: 0.000824.\nEpoch 803. Training loss: 0.001159. Validation loss: 0.000365.\nEpoch 804. Training loss: 0.001162. Validation loss: 0.000994.\nEpoch 805. Training loss: 0.001157. Validation loss: 0.000161.\nEpoch 806. Training loss: 0.001161. Validation loss: 0.003576.\nEpoch 807. Training loss: 0.001158. Validation loss: 0.002361.\nEpoch 808. Training loss: 0.001161. Validation loss: 0.000435.\nEpoch 809. Training loss: 0.001158. Validation loss: 0.002160.\nEpoch 810. Training loss: 0.001158. Validation loss: 0.001649.\nEpoch 811. Training loss: 0.001157. Validation loss: 0.001933.\nEpoch 812. Training loss: 0.001160. Validation loss: 0.001553.\nEpoch 813. Training loss: 0.001159. Validation loss: 0.001915.\nEpoch 814. Training loss: 0.001159. Validation loss: 0.001581.\nEpoch 815. Training loss: 0.001160. Validation loss: 0.003045.\nEpoch 816. Training loss: 0.001157. Validation loss: 0.003645.\nEpoch 817. Training loss: 0.001160. Validation loss: 0.002650.\nEpoch 818. Training loss: 0.001161. Validation loss: 0.000438.\nEpoch 819. Training loss: 0.001155. Validation loss: 0.000331.\nEpoch 820. Training loss: 0.001157. Validation loss: 0.000446.\nEpoch 821. Training loss: 0.001156. Validation loss: 0.000834.\nEpoch 822. Training loss: 0.001162. Validation loss: 0.000246.\nEpoch 823. Training loss: 0.001159. Validation loss: 0.002344.\nEpoch 824. Training loss: 0.001162. Validation loss: 0.000371.\nEpoch 825. Training loss: 0.001160. Validation loss: 0.000364.\nEpoch 826. Training loss: 0.001156. Validation loss: 0.000169.\nEpoch 827. Training loss: 0.001158. Validation loss: 0.000236.\nEpoch 828. Training loss: 0.001160. Validation loss: 0.003514.\nEpoch 829. Training loss: 0.001157. Validation loss: 0.002535.\nEpoch 830. Training loss: 0.001164. Validation loss: 0.000610.\nEpoch 831. Training loss: 0.001158. Validation loss: 0.000720.\nEpoch 832. Training loss: 0.001160. Validation loss: 0.000111.\nEpoch 833. Training loss: 0.001158. Validation loss: 0.002975.\nEpoch 834. Training loss: 0.001155. Validation loss: 0.003625.\nEpoch 835. Training loss: 0.001156. Validation loss: 0.000643.\nEpoch 836. Training loss: 0.001158. Validation loss: 0.000208.\nEpoch 837. Training loss: 0.001158. Validation loss: 0.000222.\nEpoch 838. Training loss: 0.001156. Validation loss: 0.000229.\nEpoch 839. Training loss: 0.001163. Validation loss: 0.000322.\nEpoch 840. Training loss: 0.001156. Validation loss: 0.001827.\nEpoch 841. Training loss: 0.001161. Validation loss: 0.001790.\nEpoch 842. Training loss: 0.001156. Validation loss: 0.003377.\nEpoch 843. Training loss: 0.001155. Validation loss: 0.001048.\nEpoch 844. Training loss: 0.001157. Validation loss: 0.000757.\nEpoch 845. Training loss: 0.001157. Validation loss: 0.005163.\nEpoch 846. Training loss: 0.001157. Validation loss: 0.001822.\nEpoch 847. Training loss: 0.001156. Validation loss: 0.000815.\nEpoch 848. Training loss: 0.001156. Validation loss: 0.000270.\nEpoch 849. Training loss: 0.001160. Validation loss: 0.001782.\nEpoch 850. Training loss: 0.001157. Validation loss: 0.002298.\nEpoch 851. Training loss: 0.001159. Validation loss: 0.001469.\nEpoch 852. Training loss: 0.001157. Validation loss: 0.002114.\nEpoch 853. Training loss: 0.001155. Validation loss: 0.003267.\nEpoch 854. Training loss: 0.001157. Validation loss: 0.000576.\nEpoch 855. Training loss: 0.001154. Validation loss: 0.000389.\nEpoch 856. Training loss: 0.001156. Validation loss: 0.002249.\nEpoch 857. Training loss: 0.001157. Validation loss: 0.001327.\nEpoch 858. Training loss: 0.001156. Validation loss: 0.001621.\nEpoch 859. Training loss: 0.001159. Validation loss: 0.003507.\nEpoch 860. Training loss: 0.001156. Validation loss: 0.001806.\nEpoch 861. Training loss: 0.001159. Validation loss: 0.002579.\nEpoch 862. Training loss: 0.001157. Validation loss: 0.000419.\nEpoch 863. Training loss: 0.001158. Validation loss: 0.002098.\nEpoch 864. Training loss: 0.001158. Validation loss: 0.001866.\nEpoch 865. Training loss: 0.001158. Validation loss: 0.000790.\nEpoch 866. Training loss: 0.001156. Validation loss: 0.001818.\nEpoch 867. Training loss: 0.001156. Validation loss: 0.001967.\nEpoch 868. Training loss: 0.001154. Validation loss: 0.001072.\nEpoch 869. Training loss: 0.001159. Validation loss: 0.001592.\nEpoch 870. Training loss: 0.001159. Validation loss: 0.000244.\nEpoch 871. Training loss: 0.001157. Validation loss: 0.001108.\nEpoch 872. Training loss: 0.001154. Validation loss: 0.000280.\nEpoch 873. Training loss: 0.001157. Validation loss: 0.004117.\nEpoch 874. Training loss: 0.001154. Validation loss: 0.000316.\nEpoch 875. Training loss: 0.001156. Validation loss: 0.001681.\nEpoch 876. Training loss: 0.001158. Validation loss: 0.000235.\nEpoch 877. Training loss: 0.001155. Validation loss: 0.000724.\nEpoch 878. Training loss: 0.001157. Validation loss: 0.001630.\nEpoch 879. Training loss: 0.001155. Validation loss: 0.000180.\nEpoch 880. Training loss: 0.001153. Validation loss: 0.000234.\nEpoch 881. Training loss: 0.001157. Validation loss: 0.000433.\nEpoch 882. Training loss: 0.001157. Validation loss: 0.000212.\nEpoch 883. Training loss: 0.001158. Validation loss: 0.001808.\nEpoch 884. Training loss: 0.001156. Validation loss: 0.001738.\nEpoch 885. Training loss: 0.001158. Validation loss: 0.000310.\nEpoch 886. Training loss: 0.001157. Validation loss: 0.000368.\nEpoch 887. Training loss: 0.001158. Validation loss: 0.002190.\nEpoch 888. Training loss: 0.001159. Validation loss: 0.000329.\nEpoch 889. Training loss: 0.001158. Validation loss: 0.001713.\nEpoch 890. Training loss: 0.001156. Validation loss: 0.001703.\nEpoch 891. Training loss: 0.001156. Validation loss: 0.001168.\nEpoch 892. Training loss: 0.001157. Validation loss: 0.000575.\nEpoch 893. Training loss: 0.001155. Validation loss: 0.000203.\nEpoch 894. Training loss: 0.001156. Validation loss: 0.003810.\nEpoch 895. Training loss: 0.001155. Validation loss: 0.003528.\nEpoch 896. Training loss: 0.001158. Validation loss: 0.004357.\nEpoch 897. Training loss: 0.001158. Validation loss: 0.000964.\nEpoch 898. Training loss: 0.001158. Validation loss: 0.003555.\nEpoch 899. Training loss: 0.001156. Validation loss: 0.000419.\nEpoch 900. Training loss: 0.001158. Validation loss: 0.002712.\nEpoch 901. Training loss: 0.001154. Validation loss: 0.000449.\nEpoch 902. Training loss: 0.001156. Validation loss: 0.001057.\nEpoch 903. Training loss: 0.001158. Validation loss: 0.000147.\nEpoch 904. Training loss: 0.001155. Validation loss: 0.003008.\nEpoch 905. Training loss: 0.001155. Validation loss: 0.001418.\nEpoch 906. Training loss: 0.001156. Validation loss: 0.001798.\nEpoch 907. Training loss: 0.001155. Validation loss: 0.002503.\nEpoch 908. Training loss: 0.001156. Validation loss: 0.001261.\nEpoch 909. Training loss: 0.001153. Validation loss: 0.000352.\nEpoch 910. Training loss: 0.001154. Validation loss: 0.001958.\nEpoch 911. Training loss: 0.001154. Validation loss: 0.000568.\nEpoch 912. Training loss: 0.001152. Validation loss: 0.000180.\nEpoch 913. Training loss: 0.001157. Validation loss: 0.000425.\nEpoch 914. Training loss: 0.001156. Validation loss: 0.000482.\nEpoch 915. Training loss: 0.001152. Validation loss: 0.000179.\nEpoch 916. Training loss: 0.001155. Validation loss: 0.002154.\nEpoch 917. Training loss: 0.001156. Validation loss: 0.003837.\nEpoch 918. Training loss: 0.001156. Validation loss: 0.000342.\nEpoch 919. Training loss: 0.001155. Validation loss: 0.000374.\nEpoch 920. Training loss: 0.001151. Validation loss: 0.002122.\nEpoch 921. Training loss: 0.001153. Validation loss: 0.002484.\nEpoch 922. Training loss: 0.001156. Validation loss: 0.001162.\nEpoch 923. Training loss: 0.001154. Validation loss: 0.000168.\nEpoch 924. Training loss: 0.001155. Validation loss: 0.001592.\nEpoch 925. Training loss: 0.001154. Validation loss: 0.000749.\nEpoch 926. Training loss: 0.001155. Validation loss: 0.001126.\nEpoch 927. Training loss: 0.001156. Validation loss: 0.003287.\nEpoch 928. Training loss: 0.001152. Validation loss: 0.003202.\nEpoch 929. Training loss: 0.001156. Validation loss: 0.000889.\nEpoch 930. Training loss: 0.001153. Validation loss: 0.001984.\nEpoch 931. Training loss: 0.001157. Validation loss: 0.002259.\nEpoch 932. Training loss: 0.001157. Validation loss: 0.003149.\nEpoch 933. Training loss: 0.001157. Validation loss: 0.002260.\nEpoch 934. Training loss: 0.001152. Validation loss: 0.001076.\nEpoch 935. Training loss: 0.001154. Validation loss: 0.000963.\nEpoch 936. Training loss: 0.001158. Validation loss: 0.000418.\nEpoch 937. Training loss: 0.001154. Validation loss: 0.000563.\nEpoch 938. Training loss: 0.001157. Validation loss: 0.002239.\nEpoch 939. Training loss: 0.001152. Validation loss: 0.003573.\nEpoch 940. Training loss: 0.001151. Validation loss: 0.000766.\nEpoch 941. Training loss: 0.001158. Validation loss: 0.000116.\nEpoch 942. Training loss: 0.001153. Validation loss: 0.001635.\nEpoch 943. Training loss: 0.001153. Validation loss: 0.000308.\nEpoch 944. Training loss: 0.001153. Validation loss: 0.000873.\nEpoch 945. Training loss: 0.001155. Validation loss: 0.000185.\nEpoch 946. Training loss: 0.001153. Validation loss: 0.003975.\nEpoch 947. Training loss: 0.001157. Validation loss: 0.002127.\nEpoch 948. Training loss: 0.001152. Validation loss: 0.000964.\nEpoch 949. Training loss: 0.001155. Validation loss: 0.002333.\nEpoch 950. Training loss: 0.001152. Validation loss: 0.001691.\nEpoch 951. Training loss: 0.001155. Validation loss: 0.001353.\nEpoch 952. Training loss: 0.001152. Validation loss: 0.000342.\nEpoch 953. Training loss: 0.001153. Validation loss: 0.000392.\nEpoch 954. Training loss: 0.001153. Validation loss: 0.000230.\nEpoch 955. Training loss: 0.001156. Validation loss: 0.000243.\nEpoch 956. Training loss: 0.001153. Validation loss: 0.006451.\nEpoch 957. Training loss: 0.001152. Validation loss: 0.000357.\nEpoch 958. Training loss: 0.001156. Validation loss: 0.003042.\nEpoch 959. Training loss: 0.001151. Validation loss: 0.000419.\nEpoch 960. Training loss: 0.001152. Validation loss: 0.001087.\nEpoch 961. Training loss: 0.001150. Validation loss: 0.000092.\nEpoch 962. Training loss: 0.001154. Validation loss: 0.000493.\nEpoch 963. Training loss: 0.001153. Validation loss: 0.000118.\nEpoch 964. Training loss: 0.001158. Validation loss: 0.000300.\nEpoch 965. Training loss: 0.001156. Validation loss: 0.001673.\nEpoch 966. Training loss: 0.001152. Validation loss: 0.000209.\nEpoch 967. Training loss: 0.001155. Validation loss: 0.000844.\nEpoch 968. Training loss: 0.001151. Validation loss: 0.001108.\nEpoch 969. Training loss: 0.001152. Validation loss: 0.000354.\nEpoch 970. Training loss: 0.001156. Validation loss: 0.004356.\nEpoch 971. Training loss: 0.001154. Validation loss: 0.000440.\nEpoch 972. Training loss: 0.001151. Validation loss: 0.000249.\nEpoch 973. Training loss: 0.001152. Validation loss: 0.000312.\nEpoch 974. Training loss: 0.001152. Validation loss: 0.002053.\nEpoch 975. Training loss: 0.001151. Validation loss: 0.001508.\nEpoch 976. Training loss: 0.001152. Validation loss: 0.002460.\nEpoch 977. Training loss: 0.001154. Validation loss: 0.000344.\nEpoch 978. Training loss: 0.001152. Validation loss: 0.001713.\nEpoch 979. Training loss: 0.001152. Validation loss: 0.001806.\nEpoch 980. Training loss: 0.001152. Validation loss: 0.000255.\nEpoch 981. Training loss: 0.001153. Validation loss: 0.002108.\nEpoch 982. Training loss: 0.001154. Validation loss: 0.000256.\nEpoch 983. Training loss: 0.001153. Validation loss: 0.002135.\nEpoch 984. Training loss: 0.001152. Validation loss: 0.002115.\nEpoch 985. Training loss: 0.001151. Validation loss: 0.001962.\nEpoch 986. Training loss: 0.001151. Validation loss: 0.002347.\nEpoch 987. Training loss: 0.001151. Validation loss: 0.000965.\nEpoch 988. Training loss: 0.001152. Validation loss: 0.000510.\nEpoch 989. Training loss: 0.001150. Validation loss: 0.000625.\nEpoch 990. Training loss: 0.001153. Validation loss: 0.000968.\nEpoch 991. Training loss: 0.001154. Validation loss: 0.000259.\nEpoch 992. Training loss: 0.001151. Validation loss: 0.002819.\nEpoch 993. Training loss: 0.001151. Validation loss: 0.000757.\nEpoch 994. Training loss: 0.001147. Validation loss: 0.000535.\nEpoch 995. Training loss: 0.001151. Validation loss: 0.001938.\nEpoch 996. Training loss: 0.001152. Validation loss: 0.001835.\nEpoch 997. Training loss: 0.001152. Validation loss: 0.000443.\nEpoch 998. Training loss: 0.001151. Validation loss: 0.000335.\nEpoch 999. Training loss: 0.001153. Validation loss: 0.003879.\nEpoch 1000. Training loss: 0.001151. Validation loss: 0.000206.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test\nmodel.load_state_dict(torch.load('best_rnn.pth'))\n\nwith torch.no_grad():\n    sum_test_loss = 0\n    total = 0\n    for i, (x_seq, targets) in enumerate(test_loader):\n        x_seq = x_seq.to(device)\n        targets = targets.to(device)\n        #targets = targets.reshape(-1, 1).to(device)\n\n        outputs = model(x_seq)\n        loss = criterion(outputs, targets)\n\n        total += targets.shape[0]\n        sum_test_loss += loss.item() * targets.shape[0]\n        #_, predicted = outputs.max(1)\n        #correct += predicted.eq(targets).sum().item()\n        \n    test_loss = sum_test_loss / total\n\nprint(test_loss)","metadata":{"id":"N_aGVAX0tgXK","outputId":"b55bb14a-1e76-4af3-9cc2-4884088abb70","trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"0.0018615394526277668\n","output_type":"stream"}]}]}