{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.utils.data as data\nimport os\nimport numpy as np\nimport json","metadata":{"id":"HnT2xwCIbLWP","trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)","metadata":{"id":"Kc3kKlk_XG7P","trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"class JsonDataset(data.Dataset):\n    def __init__(self, data_path):\n        super(JsonDataset, self).__init__()\n        \n        f = open(data_path, 'r')\n        self.data = json.loads(f.read())\n        f.close()\n        \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        return torch.FloatTensor(self.data[index][0]), \\\n            torch.LongTensor([float(self.data[index][1])])\n","metadata":{"id":"5ZUvCth2XKl2","trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"train_data = JsonDataset('/kaggle/input/cpsc490/naive2-dist-train.json')\nvalidation_data = JsonDataset('/kaggle/input/cpsc490/naive2-dist-validation.json')\ntest_data = JsonDataset('/kaggle/input/cpsc490/naive2-dist-test.json')","metadata":{"id":"ccExaxKIXs7u","trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"#x_num = train_data[0][0].shape[0]\nx_size = train_data[0][0].shape[1]\n\nbatch_size = 256\nfc_hidden_size = 200\n#fc_hidden_size_1 = 20\n#fc_hidden_size_2 = 100\n#fc_num_layers_1 = 2\n#fc_num_layers_2 = 2","metadata":{"id":"Fd4vUA9TX__H","trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': 1, 'pin_memory': True}\ntrain_loader = data.DataLoader(train_data, **params)\nvalidation_loader = data.DataLoader(validation_data, **params)\ntest_loader = data.DataLoader(test_data, **params)","metadata":{"id":"-EnaXaTUirep","trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"class Naive(nn.Module):\n    def __init__(self, x_num, x_size, \\\n                 fc_hidden_size_1, fc_hidden_size_2, \\\n                 fc_num_layers_1, fc_num_layers_2):\n        super(Naive, self).__init__()\n        \n        d = 0\n        \n        self.x_num = x_num\n        \n        seq = []\n        seq.append(nn.Linear(x_size, fc_hidden_size_1))\n        seq.append(nn.Tanh())\n        seq.append(nn.Dropout(d))\n        \n        for _ in range(fc_num_layers_1 - 1):\n            seq.append(nn.Linear(fc_hidden_size_1, fc_hidden_size_1))\n            seq.append(nn.Tanh())\n            seq.append(nn.Dropout(d))\n            \n        seq.append(nn.Linear(fc_hidden_size_1, 1))\n        self.fc1 = nn.Sequential(*seq)\n        \n        seq = []\n        seq.append(nn.Linear(x_num, fc_hidden_size_2))\n        seq.append(nn.Tanh())\n        seq.append(nn.Dropout(d))\n\n        for _ in range(fc_num_layers_2 - 1):\n            seq.append(nn.Linear(fc_hidden_size_2, fc_hidden_size_2))\n            seq.append(nn.Tanh())\n            seq.append(nn.Dropout(d))\n        \n        seq.append(nn.Linear(fc_hidden_size_2, 2))\n        self.fc2 = nn.Sequential(*seq)\n        \n    def forward(self, x):\n        layer = None\n        for i in range(self.x_num):\n            res = self.fc1(x[:, i, :])\n            \n            if layer == None:\n                layer = res\n            else:\n                layer = torch.cat((layer, res), dim=1)\n                \n        out = self.fc2(layer)\n        return out","metadata":{"id":"GrUhCI6yjfLj","trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def train(model, criterion, optimizer):\n    model.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n\n    for i, (x, targets) in enumerate(train_loader):\n        x = x.to(device)\n        targets = torch.flatten(targets).to(device)\n\n        outputs = model(x)\n        loss = criterion(outputs, targets)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total += targets.size(0)\n        train_loss += loss.item() * targets.size(0)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(targets).sum().item()\n        \n    epoch_train_loss = train_loss / total\n    epoch_train_acc = float(100 * correct / total)\n\n    return epoch_train_loss, epoch_train_acc","metadata":{"id":"toxiizvhd-s6","trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"def validation(model, criterion):\n    model.eval()\n    validation_loss = 0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for i, (x, targets) in enumerate(validation_loader):\n            x = x.to(device)\n            targets = torch.flatten(targets).to(device)\n\n            outputs = model(x)\n            loss = criterion(outputs, targets)\n\n        total += targets.size(0)\n        validation_loss += loss.item() * targets.size(0)\n        _, predicted = outputs.max(1)\n        correct += predicted.eq(targets).sum().item()\n        \n    epoch_validation_loss = validation_loss / total\n    epoch_validation_acc = float(100 * correct / total)\n\n    return epoch_validation_loss, epoch_validation_acc","metadata":{"id":"vR5S2dGD_ZTR","trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"fraction_zero = len([i for i in range(len(train_data)) if train_data[i][1] == 0]) / len(train_data)\nprint('fraction_zero: {}'.format(fraction_zero))\n\nmodel = Naive(x_num, x_size, \\\n                 fc_hidden_size_1, fc_hidden_size_2, \\\n                 fc_num_layers_1, fc_num_layers_2).to(device)\n\ncriterion = nn.CrossEntropyLoss(weight=torch.tensor([0.5 / fraction_zero, 0.5 / (1 - fraction_zero)]).to(device))\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01) \n#optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0002)\n\nnum_epochs = 200","metadata":{"id":"Sj5YBGfkgeLE","outputId":"e70c5b20-969b-45d0-a44e-1ca24d0cfba1","trusted":true},"execution_count":82,"outputs":[{"name":"stdout","text":"fraction_zero: 0.50953125\n","output_type":"stream"}]},{"cell_type":"code","source":"best_validation_loss = None\n\nfor epoch in range(0, num_epochs):\n    epoch_train_loss, epoch_train_acc = train(model, criterion, optimizer)\n    epoch_validation_loss, epoch_validation_acc = validation(\n                                                    model, criterion)\n    \n    if best_validation_loss == None or epoch_validation_loss < best_validation_loss:\n        torch.save(model.state_dict(), 'best_naive2.pth')\n        print('Saved.')\n        best_validation_loss = epoch_validation_loss\n\n    #epoch_test_loss, epoch_test_acc = test(net, criterion, vgg['best_acc'], 'vgg_best.pth')\n    #vgg['test_loss'].append(epoch_test_loss)\n    #vgg['test_acc'].append(epoch_test_acc)\n    #if epoch_test_acc > vgg['best_acc']:\n    #    vgg['best_acc'] = epoch_test_acc\n\n    print('Epoch {}. Training loss: {} ({}% accuracy). Validation loss: {} ({}% accuracy)'\n        .format(epoch + 1, \n                format(epoch_train_loss, '.4f'), format(epoch_train_acc, '.4f'),\n                format(epoch_validation_loss, '.4f'), format(epoch_validation_acc, '.4f')))","metadata":{"id":"7-n1JdS_gD8c","outputId":"c6a93907-4322-4880-ac80-7bcf9d72fd25","trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"Saved.\nEpoch 1. Training loss: 0.6991 (49.5625% accuracy). Validation loss: 0.6938 (48.4375% accuracy)\nSaved.\nEpoch 2. Training loss: 0.6941 (50.5938% accuracy). Validation loss: 0.6923 (48.4375% accuracy)\nEpoch 3. Training loss: 0.6920 (52.0625% accuracy). Validation loss: 0.7001 (45.3125% accuracy)\nSaved.\nEpoch 4. Training loss: 0.6935 (50.2188% accuracy). Validation loss: 0.6923 (54.6875% accuracy)\nEpoch 5. Training loss: 0.6925 (52.5156% accuracy). Validation loss: 0.6984 (39.0625% accuracy)\nEpoch 6. Training loss: 0.6928 (51.1875% accuracy). Validation loss: 0.6969 (45.3125% accuracy)\nSaved.\nEpoch 7. Training loss: 0.6904 (52.9844% accuracy). Validation loss: 0.6818 (57.8125% accuracy)\nEpoch 8. Training loss: 0.6886 (52.2344% accuracy). Validation loss: 0.6926 (51.5625% accuracy)\nEpoch 9. Training loss: 0.6914 (52.2969% accuracy). Validation loss: 0.6822 (51.5625% accuracy)\nEpoch 10. Training loss: 0.6894 (53.4062% accuracy). Validation loss: 0.6935 (51.5625% accuracy)\nEpoch 11. Training loss: 0.6930 (50.9531% accuracy). Validation loss: 0.6901 (48.4375% accuracy)\nEpoch 12. Training loss: 0.6864 (54.6250% accuracy). Validation loss: 0.7107 (43.7500% accuracy)\nSaved.\nEpoch 13. Training loss: 0.6843 (55.1719% accuracy). Validation loss: 0.6559 (62.5000% accuracy)\nEpoch 14. Training loss: 0.6854 (54.7656% accuracy). Validation loss: 0.7019 (54.6875% accuracy)\nEpoch 15. Training loss: 0.6847 (56.2969% accuracy). Validation loss: 0.6860 (54.6875% accuracy)\nEpoch 16. Training loss: 0.6883 (53.3906% accuracy). Validation loss: 0.7080 (43.7500% accuracy)\nEpoch 17. Training loss: 0.6850 (55.3750% accuracy). Validation loss: 0.7267 (51.5625% accuracy)\nSaved.\nEpoch 18. Training loss: 0.6877 (54.4688% accuracy). Validation loss: 0.6542 (64.0625% accuracy)\nEpoch 19. Training loss: 0.6888 (55.0000% accuracy). Validation loss: 0.6877 (59.3750% accuracy)\nEpoch 20. Training loss: 0.6875 (54.6875% accuracy). Validation loss: 0.7048 (42.1875% accuracy)\nEpoch 21. Training loss: 0.6830 (55.6719% accuracy). Validation loss: 0.6779 (59.3750% accuracy)\nEpoch 22. Training loss: 0.6849 (55.4688% accuracy). Validation loss: 0.6985 (51.5625% accuracy)\nEpoch 23. Training loss: 0.6819 (56.3750% accuracy). Validation loss: 0.6841 (50.0000% accuracy)\nEpoch 24. Training loss: 0.6805 (56.4062% accuracy). Validation loss: 0.6740 (54.6875% accuracy)\nEpoch 25. Training loss: 0.6862 (54.6875% accuracy). Validation loss: 0.7474 (40.6250% accuracy)\nEpoch 26. Training loss: 0.6843 (55.4062% accuracy). Validation loss: 0.6996 (50.0000% accuracy)\nEpoch 27. Training loss: 0.6806 (56.8906% accuracy). Validation loss: 0.6967 (48.4375% accuracy)\nEpoch 28. Training loss: 0.6820 (56.2344% accuracy). Validation loss: 0.6990 (50.0000% accuracy)\nEpoch 29. Training loss: 0.6818 (56.0312% accuracy). Validation loss: 0.6736 (59.3750% accuracy)\nEpoch 30. Training loss: 0.6775 (57.4688% accuracy). Validation loss: 0.7381 (46.8750% accuracy)\nEpoch 31. Training loss: 0.6784 (56.7812% accuracy). Validation loss: 0.6935 (51.5625% accuracy)\nEpoch 32. Training loss: 0.6778 (57.6719% accuracy). Validation loss: 0.7002 (51.5625% accuracy)\nEpoch 33. Training loss: 0.6769 (57.3750% accuracy). Validation loss: 0.6835 (53.1250% accuracy)\nEpoch 34. Training loss: 0.6778 (57.4375% accuracy). Validation loss: 0.6778 (56.2500% accuracy)\nEpoch 35. Training loss: 0.6741 (58.6875% accuracy). Validation loss: 0.7366 (48.4375% accuracy)\nEpoch 36. Training loss: 0.6758 (57.7344% accuracy). Validation loss: 0.6689 (64.0625% accuracy)\nEpoch 37. Training loss: 0.6719 (58.4219% accuracy). Validation loss: 0.7561 (48.4375% accuracy)\nEpoch 38. Training loss: 0.6712 (57.5156% accuracy). Validation loss: 0.7122 (48.4375% accuracy)\nEpoch 39. Training loss: 0.6762 (56.9375% accuracy). Validation loss: 0.7281 (48.4375% accuracy)\nEpoch 40. Training loss: 0.6790 (57.1406% accuracy). Validation loss: 0.7045 (51.5625% accuracy)\nEpoch 41. Training loss: 0.6682 (58.6562% accuracy). Validation loss: 0.7001 (54.6875% accuracy)\nEpoch 42. Training loss: 0.6688 (57.3906% accuracy). Validation loss: 0.7880 (50.0000% accuracy)\nEpoch 43. Training loss: 0.6700 (58.4844% accuracy). Validation loss: 0.7683 (35.9375% accuracy)\nEpoch 44. Training loss: 0.6674 (58.4375% accuracy). Validation loss: 0.6933 (57.8125% accuracy)\nEpoch 45. Training loss: 0.6733 (58.0938% accuracy). Validation loss: 0.7392 (46.8750% accuracy)\nEpoch 46. Training loss: 0.6693 (58.3750% accuracy). Validation loss: 0.7297 (48.4375% accuracy)\nEpoch 47. Training loss: 0.6677 (57.9219% accuracy). Validation loss: 0.7737 (45.3125% accuracy)\nEpoch 48. Training loss: 0.6686 (58.8438% accuracy). Validation loss: 0.7666 (50.0000% accuracy)\nEpoch 49. Training loss: 0.6606 (59.3281% accuracy). Validation loss: 0.7287 (51.5625% accuracy)\nEpoch 50. Training loss: 0.6578 (59.7969% accuracy). Validation loss: 0.6979 (56.2500% accuracy)\nEpoch 51. Training loss: 0.6581 (60.0156% accuracy). Validation loss: 0.7530 (42.1875% accuracy)\nEpoch 52. Training loss: 0.6551 (60.2969% accuracy). Validation loss: 0.6639 (59.3750% accuracy)\nEpoch 53. Training loss: 0.6629 (59.4062% accuracy). Validation loss: 0.7564 (53.1250% accuracy)\nEpoch 54. Training loss: 0.6619 (59.7812% accuracy). Validation loss: 0.6965 (56.2500% accuracy)\nEpoch 55. Training loss: 0.6604 (59.8125% accuracy). Validation loss: 0.8221 (45.3125% accuracy)\nEpoch 56. Training loss: 0.6576 (60.9375% accuracy). Validation loss: 0.7471 (50.0000% accuracy)\nEpoch 57. Training loss: 0.6590 (60.7812% accuracy). Validation loss: 0.7249 (53.1250% accuracy)\nEpoch 58. Training loss: 0.6515 (61.4688% accuracy). Validation loss: 0.7313 (51.5625% accuracy)\nSaved.\nEpoch 59. Training loss: 0.6556 (61.1250% accuracy). Validation loss: 0.6367 (57.8125% accuracy)\nEpoch 60. Training loss: 0.6501 (61.5156% accuracy). Validation loss: 0.7007 (62.5000% accuracy)\nEpoch 61. Training loss: 0.6419 (62.7812% accuracy). Validation loss: 0.7735 (50.0000% accuracy)\nEpoch 62. Training loss: 0.6424 (62.7969% accuracy). Validation loss: 0.6804 (57.8125% accuracy)\nEpoch 63. Training loss: 0.6363 (63.6094% accuracy). Validation loss: 0.7197 (53.1250% accuracy)\nEpoch 64. Training loss: 0.6340 (63.3438% accuracy). Validation loss: 0.8181 (42.1875% accuracy)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-83-dd6addb9d0e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mepoch_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_train_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     epoch_validation_loss, epoch_validation_acc = validation(\n\u001b[1;32m      6\u001b[0m                                                     model, criterion)\n","\u001b[0;32m<ipython-input-80-541ee96a71a2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Test\nmodel.load_state_dict(torch.load('best_naive2.pth'))\n\nwith torch.no_grad():\n    n_correct = 0\n    n_samples = 0\n        \n    for i, (x, targets) in enumerate(validation_loader):\n        x = x.to(device)\n        targets = torch.flatten(targets).to(device)\n        #targets = targets.reshape(-1, 1).to(device)\n\n        outputs = model(x)\n        _, predicted = torch.max(outputs.data, 1)\n\n        n_samples += x.shape[0]\n        n_correct += (predicted == targets).sum().item()\n    \n    acc = float(100 * n_correct / n_samples)\n    print('Test accuracy: {}%'.format(acc))","metadata":{"id":"S-dIIA2EhglE","outputId":"e7e841e3-4b15-4fb0-ad79-0fed74c7cdd2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hit = 0\npositives = 0\nn = 0\nfor i in range(len(test_data)):\n    x_center, real, song, target = test_data[i]\n    \n    target = (target == 1)\n    found = False\n    for c in x_center:\n        if c.tolist() == song.tolist():\n            found = True\n    \n    #if found:\n    #    n += 1\n            \n    if found == target:\n        hit += 1\n\nprint(100.0 * hit / len(test_data))\n#print(hit)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\ncandidates = {'gmat': [780,750,690,710,780,730,690,720,740,690,610,690,710,680,770,610,580,650,540,590,620,600,550,550,570,670,660,580,650,760,640,620,660,660,680,650,670,580,590,790],\n              'gpa': [4,3.9,3.3,3.7,3.9,3.7,2.3,3.3,3.3,1.7,2.7,3.7,3.7,3.3,3.3,3,2.7,3.7,2.7,2.3,3.3,2,2.3,2.7,3,3.3,3.7,2.3,3.7,3.3,3,2.7,4,3.3,3.3,2.3,2.7,3.3,1.7,3.7],\n              'work_experience': [3,4,3,5,4,6,1,4,5,1,3,5,6,4,3,1,4,6,2,3,2,1,4,1,2,6,4,2,6,5,1,2,4,6,5,1,2,1,4,5],\n              'age': [25,28,24,27,26,31,24,25,28,23,25,27,30,28,26,23,29,31,26,26,25,24,28,23,25,29,28,26,30,30,23,24,27,29,28,22,23,24,28,31],\n              'admitted': [2,2,1,2,2,2,0,2,2,0,0,2,2,1,2,0,0,1,0,0,1,0,0,0,0,1,1,0,1,2,0,0,1,1,1,0,0,0,0,2]\n              }\n\nfor i in len(my_data[0])\n\n\ndf = pd.DataFrame(my_data)\n\nX = df[['gmat', 'gpa','work_experience','age']]\ny = df['admitted']\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)\n\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train,y_train)\ny_pred=clf.predict(X_test)\n\nconfusion_matrix = pd.crosstab(y_test, y_pred, rownames=['Actual'], colnames=['Predicted'])\nsn.heatmap(confusion_matrix, annot=True)\n\nprint('Accuracy: ',metrics.accuracy_score(y_test, y_pred))\nplt.show()\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_data = [item[0] + [item[1]] for item in my_data]\npd.DataFrame(my_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}